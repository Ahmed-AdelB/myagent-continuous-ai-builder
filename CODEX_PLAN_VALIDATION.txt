**Plan Review (TRI_AGENT_COMPREHENSIVE_PLAN.md)**  
- Feasibility: Scope (15.6k new LOC + 3.2k mods, 5 weeks) is aggressive for tri-agent; Phase 0 internet research and 1M-token analyses assume unrestricted network/context—current env and model constraints may block. RAG-first is correct, but inventory+gap JSONs at 1M tokens remain brittle without chunking/metadata strategy and human checkpoints. Deliverable sizes (5k-word/4k-word reports, 247 citations) look aspirational vs. time. “Autonomous” research may violate network/safety policy—needs explicit allowance and fallbacks.  
- Architecture/Design: Roles are specified but routing is capability-only; add cost/latency/queue-backpressure signals and circuit breakers. Conflict resolution exists but no clear ownership for tie-break or SLA; define lead agent + timeout-based quorum. Validation stack (self+peer+tests+static) is strong, yet orchestration lacks idempotent task state and retry policy; add durable task ledger with statuses. RAG is prioritized but architecture missing: corpus definition (code, docs, tickets), chunking (semantic + hierarchical), embeddings model choice, retrieval routing per agent, and evaluation. GeminiSession wrapper mentioned but not codified (persistence, key rotation, redaction).  
- Testing: Good breadth (property, mutation, contract, chaos) but no gating order; propose fast unit/contract smoke on every change, mutation nightly, chaos per sprint. No explicit coverage targets per component nor non-functional tests (perf, load, latency budgets). Static analysis is named but stack missing (ruff/mypy/bandit/trivy/gitleaks/kics) and enforcement points (pre-commit/CI stages). No deterministic seeds/fixtures for probabilistic tests; add them.  
- Debt/risks/anti-patterns: Over-reliance on single-pass “1M token” reads (model/latency costs, context bleed). Waterfall-ish artifact bloat despite “sprints”; risk of report generation overshadowing delivery. No data governance (PII/secrets redaction, dataset versioning, retention) for RAG or logs. Tooling not pinned (versions, reproducible envs) nor supply-chain checks (SBOM, sigstore). Missing observability for agents (structured logs, traces, metrics with SLOs). Task routing ignores dependency graph; potential deadlocks/priority inversion. No rollback/feature-flag plan for applied changes.  

Recommendations (actionable)  
1) RAG architecture now: define corpus (code+docs), chunking (code-aware split + function-aware), embeddings model, metadata schema, retrieval eval set, and latency/quality KPIs; implement ingestion CI job + drift alerts.  
2) Orchestration hardening: add task ledger (states: queued/running/retry/blocked/done), retries with jitter, timeouts, cancellation, routing signals (capability + load + cost + historical win-rate), and explicit tie-breaker owner + quorum rule.  
3) CI/testing pipeline: codify stages—pre-commit (fmt/ruff/mypy/bandit/gitleaks), PR CI (unit+contracts+baseline perf), nightly (mutation/property fuzz subset), weekly (chaos/load). Add deterministic seeds and coverage gates per service.  
4) Security/Supply chain: add SBOM generation + vuln scan, dependency pinning/renovate cadence, secret scanning on PR, infra scanning (trivy/kics) for Docker/K8s manifests, and policy-as-code gates.  
5) Observability: structured JSON logs per agent, trace spans for tasks, metrics (latency, success rate, queue depth, cost), SLOs with alerting; capture provenance of each deliverable (inputs, model, hash).  
6) Data governance: redaction for prompts/logs, retention policy, dataset/catalog for RAG with versioned snapshots; access controls and key rotation baked into GeminiSession wrapper.  
7) Delivery focus: trim report bloat; prioritize working increments each sprint (RAG core → routing → validation gates → incremental inventory). Add feature flags/rollback plan for agent-driven changes.  
8) Feasibility adjust: revisit timeline/LOC targets; stage rollouts and secure approvals for network-heavy research; simulate with offline corpora when blocked.