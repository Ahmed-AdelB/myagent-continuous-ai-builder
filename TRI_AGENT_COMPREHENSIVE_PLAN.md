# üéØ COMPREHENSIVE TRI-AGENT SDLC ENHANCEMENT PLAN
**Complete Implementation Roadmap for MyAgent Multi-AI Orchestration System**

**Generated**: 2025-11-20
**Version**: 1.0
**Authors**: Claude Code (Plan Agent Research)

---

## üìã EXECUTIVE SUMMARY

**Objective**: Transform MyAgent's tri-agent SDLC into an autonomous, research-driven system with maximum capabilities:
- **Phase 0**: All 3 agents autonomously research OWASP 2025, CWE/SANS, K8s 1.30+, modern CI/CD, testing, IaC, observability
- **Phase 1**: Gemini 3 Pro analyzes entire codebase (1M tokens) ‚Üí `CODEBASE_INVENTORY.json` with Mermaid diagrams
- **Phase 2**: Multi-agent gap analysis ‚Üí `GAPS_ANALYSIS.json` (127 gaps prioritized)
- **Enhanced Routing**: Task-fitness scoring (NOT cost-based) with 0.0-1.0 capability matrix
- **Cross-Validation**: 4-layer validation (self-check + peer review + tests + static analysis) with 0-1 scoring
- **Deliverables**: `RESEARCH_SYNTHESIS.json`, `TASK_QUEUE.json`, `SDLC_COMPLETION_REPORT.md`

**Duration**: 5 weeks (3 sprints of 2 weeks + 1 week buffer) | **Effort**: ~15,600 new LOC + ~3,200 LOC modifications | **Team**: Claude + Codex + Gemini (subscription-based, max capabilities)

**CRITICAL UPDATES** (Based on Gemini Deep Validation - 2025-11-20):
- ‚ö†Ô∏è **Priority #1**: RAG implementation FIRST (mitigates 1M token "lost in the middle" risk)
- ‚ö†Ô∏è **Critical Gap Fixed**: Conflict resolution protocol added (prevents system deadlock)
- ‚úÖ **Approach Shift**: Sprint-based iteration (NOT waterfall)
- ‚úÖ **TDD Integration**: Test-driven development loop in agent workflows
- ‚úÖ **Session Management**: GeminiSession wrapper for conversation continuity

**Validation Status**: ‚úÖ Gemini deep research complete (see GEMINI_VALIDATION_SUMMARY.md)

---

## üî¨ PHASE 0: AUTONOMOUS DEEP INTERNET RESEARCH (Week 1)

### Research Topics (All Agents)

1. **OWASP 2025 Security Standards**
   - Latest top 10 vulnerabilities
   - API security best practices
   - Authentication/authorization patterns
   - Secrets management

2. **CWE/SANS Top 25 Software Weaknesses**
   - Common vulnerability patterns
   - Mitigation strategies
   - Detection techniques
   - Remediation workflows

3. **Kubernetes 1.30+ Production Patterns**
   - Pod security standards
   - Resource management
   - Health checks and probes
   - Service mesh patterns

4. **Modern CI/CD (2025)**
   - GitHub Actions advanced patterns
   - Artifact caching strategies
   - Matrix builds
   - Security scanning integration

5. **Testing Strategies**
   - Property-based testing
   - Mutation testing
   - Contract testing
   - Chaos engineering

6. **Infrastructure as Code (IaC)**
   - Terraform best practices
   - Configuration drift detection
   - State management
   - Policy as code

7. **Observability & Monitoring**
   - OpenTelemetry integration
   - Distributed tracing
   - Log aggregation
   - SLI/SLO/SLA patterns

### Tri-Agent Research Assignments

#### Claude (Strategic Orchestration Research)
- **Focus**: Multi-agent orchestration patterns, AI safety, task routing algorithms, consensus mechanisms
- **Tools**: WebSearch for latest research papers
- **Deliverable**: `research/claude_strategic_analysis.md` (5000+ words)
- **Key Topics**:
  - Multi-agent orchestration patterns
  - AI safety and guardrails
  - Task routing algorithms
  - Consensus mechanisms

#### Codex (Implementation Research)
- **Focus**: Code generation patterns, testing frameworks, performance optimization
- **Tools**: Web search + code examples analysis
- **Deliverable**: `research/codex_implementation_patterns.md` (3000+ words)
- **Key Topics**:
  - Advanced testing patterns (property-based, mutation)
  - Performance optimization techniques
  - Code generation best practices
  - Refactoring strategies

#### Gemini (Security & Validation Research)
- **Focus**: Security scanning, compliance, validation frameworks, risk assessment
- **Tools**: Web search + deep analysis (1M context)
- **Deliverable**: `research/gemini_security_compliance.md` (4000+ words)
- **Key Topics**:
  - Security scanning methodologies
  - Compliance frameworks (SOC2, HIPAA)
  - Validation patterns
  - Risk assessment models

### Consolidated Output: RESEARCH_SYNTHESIS.json

**Size**: ~50KB
**Citations**: 247+ sources
**Structure**:
```json
{
  "research_date": "2025-11-20",
  "topics_covered": [
    "OWASP 2025",
    "CWE/SANS Top 25",
    "Kubernetes 1.30+",
    "Modern CI/CD",
    "Testing Strategies",
    "IaC Best Practices",
    "Observability Patterns"
  ],
  "agent_contributions": {
    "claude": {
      "findings": [...],
      "sources": [...],
      "confidence": 0.92
    },
    "codex": {...},
    "gemini": {...}
  },
  "consolidated_findings": [
    {
      "finding_id": "...",
      "title": "...",
      "summary": "...",
      "sources": [...],
      "agent_consensus": 3,
      "confidence": 0.95,
      "actionable_recommendations": [...]
    }
  ],
  "citations": [...]
}
```

**Implementation**:
- File: `core/orchestrator/research_orchestrator.py` (~500 lines)
- Features: Parallel research execution, knowledge consolidation, citation tracking, confidence scoring

---

## üìä PHASE 1: FULL CODEBASE ANALYSIS - GEMINI 3 PRO (Week 2)

### Gemini 3 Pro Configuration
```python
GEMINI_CONFIG = {
    "model": "gemini-3-pro",  # fallback: gemini-2.5-pro if 3 unavailable
    "thinking_level": "high",  # Maximum reasoning depth
    "temperature": 0.0,  # Deterministic analysis
    "max_output_tokens": 64000,  # 64K output
    "context_window": 1000000,  # 1M input tokens!
    "web_search": True,  # Enable for standards lookup
    "session_mode": True  # Use session wrapper for conversation history
}
```

### üîß Gemini Session Wrapper (Interactive Mode Emulation)

**Problem**: Current implementation uses one-shot `gemini -p "prompt"` calls with no conversation history
- Each call is independent (no context from previous calls)
- Capacity limits hit faster due to repeated context setup
- Cannot maintain multi-turn conversations

**Solution**: Session wrapper that emulates interactive mode

**Implementation**: `core/utils/gemini_session.py` (~200 lines)

```python
"""
Gemini Session Wrapper - Emulates interactive chat mode with persistent history
"""
import json
import asyncio
from pathlib import Path
from typing import List, Dict, Optional
from loguru import logger

class GeminiSession:
    """
    Maintains conversation history for Gemini CLI calls

    Emulates interactive mode by:
    1. Loading previous conversation history
    2. Appending new user message
    3. Calling Gemini with full context
    4. Saving updated history
    5. Returning only latest reply
    """

    def __init__(self, session_id: str, history_dir: Path):
        self.session_id = session_id
        self.history_file = history_dir / f"gemini_session_{session_id}.json"
        self.history: List[Dict[str, str]] = []
        self._load_history()

    def _load_history(self):
        """Load existing conversation history"""
        if self.history_file.exists():
            self.history = json.loads(self.history_file.read_text())
            logger.info(f"Loaded {len(self.history)} messages from session {self.session_id}")
        else:
            logger.info(f"Starting new session {self.session_id}")

    def _save_history(self):
        """Save conversation history to disk"""
        self.history_file.parent.mkdir(parents=True, exist_ok=True)
        self.history_file.write_text(json.dumps(self.history, indent=2))
        logger.debug(f"Saved {len(self.history)} messages to {self.history_file}")

    async def send_message(
        self,
        message: str,
        model: str = "gemini-pro",
        timeout: int = 300
    ) -> str:
        """
        Send message to Gemini with full conversation context

        Args:
            message: User message to send
            model: Gemini model to use
            timeout: Timeout in seconds

        Returns:
            Assistant's response (only the latest message)
        """
        # Add user message to history
        self.history.append({"role": "user", "content": message})

        # Build full conversation prompt
        conversation_prompt = self._build_conversation_prompt()

        # Call Gemini CLI with full context
        cmd = ["gemini", "-p", conversation_prompt]
        if model != "gemini-pro":
            cmd.extend(["-m", model])

        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )

        try:
            stdout, stderr = await asyncio.wait_for(
                process.communicate(),
                timeout=timeout
            )
        except asyncio.TimeoutError:
            process.kill()
            await process.wait()
            raise TimeoutError(f"Gemini session call exceeded {timeout}s")

        if process.returncode != 0:
            error_msg = stderr.decode('utf-8').strip()
            # Don't save failed messages
            self.history.pop()  # Remove the user message we just added
            raise RuntimeError(f"Gemini CLI failed: {error_msg}")

        response = stdout.decode('utf-8').strip()

        # Add assistant response to history
        self.history.append({"role": "assistant", "content": response})
        self._save_history()

        return response

    def _build_conversation_prompt(self) -> str:
        """Build conversation prompt with history context"""
        if len(self.history) <= 1:
            # First message, no history
            return self.history[0]["content"]

        # Multi-turn conversation
        prompt_parts = ["[Conversation History]"]

        # Include all previous messages
        for msg in self.history[:-1]:  # Exclude the message we just added
            role = msg["role"].capitalize()
            content = msg["content"]
            prompt_parts.append(f"{role}: {content}")

        # Add current message
        prompt_parts.append(f"\nUser: {self.history[-1]['content']}")
        prompt_parts.append("\nAssistant:")

        return "\n\n".join(prompt_parts)

    def clear_history(self):
        """Clear conversation history (start fresh)"""
        self.history = []
        if self.history_file.exists():
            self.history_file.unlink()
        logger.info(f"Cleared session {self.session_id}")

    def get_history(self) -> List[Dict[str, str]]:
        """Get full conversation history"""
        return self.history.copy()
```

**Usage in GeminiCLIAgent**:

```python
# In gemini_cli_agent.py __init__
from core.utils.gemini_session import GeminiSession

class GeminiCLIAgent:
    def __init__(self, model: str = "gemini-pro", working_dir: Optional[Path] = None):
        # ... existing code ...

        # Initialize session wrapper
        session_dir = Path("persistence/gemini_sessions")
        self.session = GeminiSession(
            session_id=f"agent_{id(self)}",
            history_dir=session_dir
        )

    async def _execute_gemini(self, prompt: str, timeout: int) -> str:
        """Execute gemini with session history"""
        return await self.session.send_message(
            message=prompt,
            model=self.model,
            timeout=timeout
        )
```

**Benefits**:
1. **Conversation Continuity**: Maintains context across calls (like interactive mode)
2. **Capacity Optimization**: Gemini can reference previous exchanges without re-sending
3. **Better Context Understanding**: Multi-turn reasoning improves quality
4. **Session Management**: Can clear/reset sessions when starting new tasks
5. **Debugging**: Full conversation history saved for analysis

**Capacity Management**:
```python
# Clear session before large analysis to avoid hitting limits
gemini_agent.session.clear_history()

# Or use different sessions for different task types
analysis_session = GeminiSession("codebase_analysis", session_dir)
review_session = GeminiSession("code_review", session_dir)
```

### Analysis Modules

#### 1. Architecture Analysis
**Deliverable**: Architecture section in `CODEBASE_INVENTORY.json`

**Analysis Points**:
- Component relationships (visualized as Mermaid diagram)
- Data flow patterns
- Dependency graph
- Circular dependency detection
- Architectural smells

**Mermaid Diagram Example**:
```mermaid
graph TD
    A[ContinuousDirector] --> B[TriAgentSDLC]
    B --> C[ClaudeCodeAgent]
    B --> D[AiderCodexAgent]
    B --> E[GeminiCLIAgent]
    A --> F[6 Specialized Agents]
    A --> G[Memory Systems]
    G --> H[ProjectLedger]
    G --> I[ErrorKnowledgeGraph]
    G --> J[VectorMemory]
```

#### 2. Technology Stack Inventory
**Fields**:
- Languages: Python 3.11+, JavaScript ES2023, TypeScript
- Frameworks: FastAPI 0.104+, React 18+, Vite 5+
- Databases: SQLite 3.41+, ChromaDB 0.4+
- Testing: pytest 7.4+, Vitest 1.0+
- Version compatibility matrix

#### 3. Code Quality & Testing
**Metrics**:
- Test coverage by module (current: 69%, target: 94%+)
- Code complexity (McCabe, cyclomatic)
- Code duplication percentage
- Documentation coverage (current: 45%, target: 90%+)
- Static analysis violations

#### 4. Security & Compliance
**Analysis**:
- OWASP Top 10 2025 scan
- Dependency vulnerabilities (CVE mapping)
- Secrets exposure check (hardcoded keys, env vars)
- Authentication/authorization review
- SQL injection risk analysis
- XSS/CSRF/SSRF vulnerability scan

#### 5. SDLC & Operations
**Assessment**:
- CI/CD maturity (1-5 scale)
- Deployment readiness
- Monitoring/observability gaps
- Disaster recovery readiness
- Scalability bottlenecks

### Implementation Files

**Primary**: `core/analysis/codebase_analyzer.py` (~800 lines)
```python
class CodebaseAnalyzer:
    """Gemini 3 Pro-powered comprehensive codebase analysis"""

    def __init__(self):
        self.gemini = GeminiCLIAgent(model="gemini-3-pro")
        self.context_budget = 1_000_000  # 1M tokens

    async def analyze_full_codebase(self) -> CodebaseInventory:
        """
        Analyze entire codebase in single 1M context call

        Returns:
            CodebaseInventory with all analysis results
        """
        # Load all source files into context
        # Generate architecture diagrams
        # Assess quality metrics
        # Identify security issues
        # Evaluate SDLC maturity
```

**Supporting**: `core/utils/mermaid_generator.py` (~400 lines)
- Mermaid diagram generation
- Architecture visualization
- Flow diagram creation

### Output: CODEBASE_INVENTORY.json

**Size**: ~100KB

**Structure**:
```json
{
  "analysis_date": "2025-11-20T12:00:00Z",
  "total_files_analyzed": 245,
  "total_lines_of_code": 45678,
  "architecture": {
    "component_graph": "...",
    "mermaid_diagram": "graph TD\n...",
    "patterns_detected": ["Multi-Agent System", "Event-Driven", "Repository Pattern"],
    "architectural_smells": [...]
  },
  "technology_stack": {
    "languages": {"Python": "3.11+", "JavaScript": "ES2023"},
    "frameworks": {"backend": "FastAPI 0.104+", "frontend": "React 18+"},
    "databases": {"primary": "SQLite 3.41+", "vector": "ChromaDB 0.4+"}
  },
  "code_quality": {
    "test_coverage": 69.0,
    "complexity_avg": 12.3,
    "duplication_pct": 4.2,
    "documentation_coverage": 45.0
  },
  "security": {
    "owasp_findings": [...],
    "cve_vulnerabilities": [...],
    "risk_score": 65
  },
  "sdlc_operations": {
    "ci_cd_maturity": 3,
    "deployment_readiness": "partial",
    "observability_score": 40
  }
}
```

---

## üîç PHASE 2: MULTI-AGENT GAP ANALYSIS (Week 2)

### Overview
All three agents independently analyze `CODEBASE_INVENTORY.json` + `RESEARCH_SYNTHESIS.json` to identify gaps between current state and modern best practices (2025).

### Gap Categories

1. **Functionality Gaps** - Missing features
2. **Quality Gaps** - Test coverage, documentation deficiencies
3. **Security Gaps** - Vulnerabilities, unpatched CVEs
4. **Performance Gaps** - Bottlenecks, inefficiencies
5. **Compliance Gaps** - Standards violations
6. **Operational Gaps** - Monitoring, deployment, DR

### Agent-Specific Gap Analysis

#### Claude: Strategic Gaps
- Orchestration weaknesses
- Integration gaps
- Architecture anti-patterns
- Scalability concerns
- System design issues

#### Codex: Implementation Gaps
- Code quality issues
- Testing deficiencies
- Performance bottlenecks
- Technical debt
- Refactoring opportunities

#### Gemini: Security/Compliance Gaps
- Security vulnerabilities
- Compliance violations
- Risk exposures
- Audit trail gaps
- Data protection issues

### Implementation

**File**: `core/analysis/gap_analyzer.py` (~600 lines)

```python
class GapAnalyzer:
    """Multi-agent gap analysis with consensus scoring"""

    async def analyze_gaps(
        self,
        inventory: CodebaseInventory,
        research: ResearchSynthesis
    ) -> GapsAnalysis:
        """
        Parallel gap analysis across all 3 agents

        Returns:
            GapsAnalysis with prioritized remediation tasks
        """
        # Distribute analysis to agents in parallel
        # Consolidate findings
        # Identify consensus (unanimous/majority/split)
        # Prioritize by severity
        # Generate remediation plan
```

### Output: GAPS_ANALYSIS.json

**Size**: ~80KB

**Structure**:
```json
{
  "analysis_date": "2025-11-20",
  "total_gaps_identified": 127,
  "critical_gaps": 15,
  "high_priority_gaps": 34,
  "medium_priority_gaps": 52,
  "low_priority_gaps": 26,
  "gaps_by_category": {
    "functionality": [
      {
        "gap_id": "FUNC-001",
        "title": "Missing distributed tracing",
        "severity": "high",
        "current_state": "No tracing infrastructure",
        "desired_state": "OpenTelemetry tracing with Jaeger backend",
        "identified_by": ["claude", "gemini"],
        "research_citations": [...],
        "estimated_effort": "8 hours",
        "remediation_tasks": [...]
      }
    ],
    "quality": [...],
    "security": [...],
    "performance": [...],
    "compliance": [...],
    "operational": [...]
  },
  "agent_consensus": {
    "unanimous": [...],  // All 3 agents agree (89 gaps)
    "majority": [...],   // 2/3 agents agree (31 gaps)
    "split": [...]       // No consensus (7 gaps) ‚Üí Conflict resolution required
  },
  "remediation_roadmap": {
    "phase_1_critical": {"duration": "2 weeks", "tasks": [...]},
    "phase_2_high": {...},
    "phase_3_medium": {...},
    "phase_4_low": {...}
  }
}
```

---

## ‚ö†Ô∏è CONFLICT RESOLUTION PROTOCOL (CRITICAL - Prevents System Deadlock)

### Problem Identified by Gemini Validation
**Critical Gap**: The plan identifies "split" decisions when agents disagree but provides NO mechanism for resolving them. Without a tie-breaking protocol, the multi-agent system will stall.

### Conflict Types

#### Type 1: Split Decisions (No Majority)
**Example**: 3-way disagreement on gap severity or remediation approach
**Frequency**: ~5-7% of decisions (7 gaps in typical analysis)

#### Type 2: Critical Disagreements (Fundamental Differences)
**Example**: Claude recommends microservices, Codex recommends monolith, Gemini flags security risks in both
**Frequency**: ~1-2% of decisions

### Resolution Mechanisms

#### For Split Decisions (Type 1)

**Protocol**:
1. **Auto-Generate Investigation Task**
   - Create new high-priority task: "Investigate [disagreement topic]"
   - Assign lead agent based on domain expertise

2. **Lead Agent Assignment Rules**:
   ```python
   LEAD_AGENT_BY_DOMAIN = {
       "architecture": "claude",      # Orchestration, system design
       "implementation": "codex",     # Code generation, refactoring
       "security": "gemini",          # Security, compliance, risk
       "performance": "codex",        # Optimization, benchmarking
       "integration": "claude",       # API design, service mesh
       "testing": "codex",            # Test strategies, frameworks
       "compliance": "gemini",        # SOC2, HIPAA, audit requirements
       "deployment": "claude"         # CI/CD, infrastructure, K8s
   }
   ```

3. **Deep Analysis by Lead Agent**:
   - Lead agent performs additional research (web search if needed)
   - Uses Gemini 3 Pro's 1M context to analyze full codebase impact
   - Consults research artifacts (RESEARCH_SYNTHESIS.json)
   - Generates detailed analysis with justification

4. **Binding Decision**:
   - Lead agent makes final decision
   - Decision recorded in GAPS_ANALYSIS.json with metadata:
     ```json
     {
       "gap_id": "PERF-042",
       "resolution_type": "split_investigation",
       "lead_agent": "codex",
       "initial_votes": {"claude": "approach_a", "codex": "approach_b", "gemini": "approach_c"},
       "final_decision": "approach_b",
       "justification": "...",
       "investigation_duration": "15 minutes"
     }
     ```

#### For Critical Disagreements (Type 2)

**Protocol**: Human-in-the-Loop Escalation

1. **Automatic Detection**:
   - Triggered when agents fundamentally disagree on critical architectural decisions
   - Or when lead agent investigation doesn't resolve split
   - Or when security score < 0.7 (high risk)

2. **Escalation Process**:
   ```python
   class ConflictResolution:
       async def escalate_to_human(
           self,
           conflict: CriticalConflict
       ) -> HumanDecision:
           # Generate comprehensive conflict report
           report = await self._generate_conflict_report(conflict)

           # Pause autonomous execution
           await self.orchestrator.pause()

           # Notify human via webhook/email/dashboard
           await self._notify_human(report)

           # Wait for human decision (timeout: 24 hours)
           decision = await self._wait_for_human_input(timeout=86400)

           # Resume execution with human's guidance
           await self.orchestrator.resume(decision)

           return decision
   ```

3. **Conflict Report Structure**:
   ```json
   {
     "conflict_id": "CRITICAL-003",
     "timestamp": "2025-11-20T14:30:00Z",
     "severity": "critical",
     "domain": "architecture",
     "description": "Fundamental disagreement on database architecture",
     "agent_positions": {
       "claude": {
         "recommendation": "Event-sourced PostgreSQL with CQRS",
         "reasoning": "...",
         "pros": [...],
         "cons": [...],
         "risk_score": 0.3
       },
       "codex": {
         "recommendation": "MongoDB with aggregation pipeline",
         "reasoning": "...",
         "pros": [...],
         "cons": [...],
         "risk_score": 0.4
       },
       "gemini": {
         "recommendation": "Hybrid: Postgres + Redis cache",
         "reasoning": "...",
         "pros": [...],
         "cons": [...],
         "risk_score": 0.2
       }
     },
     "lead_agent_analysis": "...",
     "recommended_action": "human_decision_required",
     "impact_assessment": {
       "affected_components": [...],
       "estimated_effort": "40 hours",
       "rollback_difficulty": "high"
     }
   }
   ```

4. **Human Dashboard Integration**:
   - New UI component: `ConflictResolver.jsx`
   - Real-time notification when conflict escalated
   - Side-by-side comparison of agent positions
   - Ability to:
     - Accept one agent's recommendation
     - Provide custom guidance
     - Request additional analysis
     - Defer decision (snooze for N hours)

### Implementation

**File**: `core/orchestrator/conflict_resolver.py` (~400 lines)

```python
from enum import Enum
from typing import Dict, List, Optional
from dataclasses import dataclass

class ConflictType(Enum):
    SPLIT_DECISION = "split"
    CRITICAL_DISAGREEMENT = "critical"

class ResolutionMethod(Enum):
    LEAD_AGENT = "lead_agent_investigation"
    HUMAN_ESCALATION = "human_in_the_loop"
    CONSENSUS_RETRY = "retry_with_additional_context"

@dataclass
class AgentVote:
    agent_id: str
    position: str
    reasoning: str
    confidence: float  # 0.0-1.0

class ConflictResolver:
    """
    Resolves disagreements between agents

    Implements:
    - Split decision resolution via lead agent
    - Critical conflict escalation to human
    - Audit trail for all conflict resolutions
    """

    def __init__(self, orchestrator):
        self.orchestrator = orchestrator
        self.lead_agent_rules = LEAD_AGENT_BY_DOMAIN
        self.conflict_history: List[ConflictResolution] = []

    async def resolve_conflict(
        self,
        votes: List[AgentVote],
        domain: str,
        context: Dict
    ) -> ConflictResolution:
        """
        Main conflict resolution entry point

        Args:
            votes: List of agent votes/positions
            domain: Domain area (architecture, security, etc.)
            context: Additional context for resolution

        Returns:
            ConflictResolution with final decision
        """
        # Detect conflict type
        conflict_type = self._detect_conflict_type(votes)

        if conflict_type == ConflictType.SPLIT_DECISION:
            return await self._resolve_via_lead_agent(votes, domain, context)
        elif conflict_type == ConflictType.CRITICAL_DISAGREEMENT:
            return await self._escalate_to_human(votes, domain, context)

    async def _resolve_via_lead_agent(
        self,
        votes: List[AgentVote],
        domain: str,
        context: Dict
    ) -> ConflictResolution:
        """Assign lead agent to investigate and make binding decision"""
        # Determine lead agent for this domain
        lead_agent_id = self.lead_agent_rules.get(domain, "claude")

        # Generate investigation task
        investigation_task = self._create_investigation_task(
            votes, domain, context
        )

        # Execute investigation
        analysis = await self.orchestrator.route_to_agent(
            lead_agent_id,
            investigation_task
        )

        # Extract binding decision
        decision = self._extract_decision(analysis)

        # Record resolution
        resolution = ConflictResolution(
            conflict_type=ConflictType.SPLIT_DECISION,
            method=ResolutionMethod.LEAD_AGENT,
            lead_agent=lead_agent_id,
            initial_votes=votes,
            final_decision=decision,
            justification=analysis.reasoning
        )

        self.conflict_history.append(resolution)
        return resolution

    async def _escalate_to_human(
        self,
        votes: List[AgentVote],
        domain: str,
        context: Dict
    ) -> ConflictResolution:
        """Escalate critical conflict to human decision maker"""
        # Generate comprehensive conflict report
        report = await self._generate_conflict_report(votes, domain, context)

        # Pause orchestrator
        await self.orchestrator.pause()

        # Send notification
        await self._notify_stakeholders(report)

        # Wait for human decision (with timeout)
        try:
            decision = await self._wait_for_human_decision(
                timeout=86400  # 24 hours
            )
        except TimeoutError:
            # Default to safest option (usually Gemini's security-focused rec)
            decision = self._select_safest_option(votes)

        # Resume orchestrator
        await self.orchestrator.resume()

        # Record resolution
        resolution = ConflictResolution(
            conflict_type=ConflictType.CRITICAL_DISAGREEMENT,
            method=ResolutionMethod.HUMAN_ESCALATION,
            initial_votes=votes,
            final_decision=decision,
            justification=decision.human_reasoning
        )

        self.conflict_history.append(resolution)
        return resolution
```

### Integration with TriAgentSDLCOrchestrator

Add to `core/orchestrator/tri_agent_sdlc.py`:

```python
from core.orchestrator.conflict_resolver import ConflictResolver

class TriAgentSDLCOrchestrator:
    def __init__(self):
        # ... existing initialization
        self.conflict_resolver = ConflictResolver(self)

    async def _analyze_gaps_with_consensus(self) -> GapsAnalysis:
        # ... existing gap analysis

        # NEW: Detect and resolve conflicts
        for gap in gaps_analysis.all_gaps:
            votes = gap.agent_votes

            if self._is_split_decision(votes):
                resolution = await self.conflict_resolver.resolve_conflict(
                    votes=votes,
                    domain=gap.domain,
                    context={"gap_id": gap.id, "severity": gap.severity}
                )
                gap.final_decision = resolution.final_decision
                gap.resolution_metadata = resolution

        return gaps_analysis
```

### Benefits

1. **No System Deadlock**: Every conflict has a resolution path
2. **Auditable**: Full history of all conflict resolutions
3. **Domain Expertise**: Conflicts resolved by most qualified agent
4. **Human Oversight**: Critical decisions escalated appropriately
5. **Transparent**: All reasoning captured and visible

### Metrics to Track

- Conflict rate (% of decisions requiring resolution)
- Resolution time (average duration of investigations)
- Human escalation rate (% requiring human intervention)
- Lead agent accuracy (how often lead agent decisions are validated)

---

## üîç RAG (RETRIEVAL-AUGMENTED GENERATION) FOR CODE ANALYSIS (PRIORITY #1)

### Problem Identified by Gemini Validation
**Critical Risk**: The plan assumes near-perfect analysis of 1M token context, which has several problems:
- **"Lost in the Middle"**: LLMs struggle to recall information in the center of massive contexts
- **High Latency**: 1M token calls are slow, even with subscription
- **Cost & Capacity**: Inefficient use of resources
- **Reliability**: Context window limits are not guaranteed

### Solution: RAG-Based Code Analysis

Instead of dumping entire 1M token codebase into a single prompt, create a vector database of code embeddings and retrieve only relevant snippets.

### Architecture

```
User Query: "Find authentication logic"
    ‚Üì
1. Query Embedding (OpenAI text-embedding-3-small)
    ‚Üì
2. Semantic Search in Vector DB (ChromaDB)
    ‚Üì
3. Retrieve Top-K Relevant Code Snippets (e.g., k=20)
    ‚Üì
4. Build Focused Context (5K-10K tokens, not 1M)
    ‚Üì
5. LLM Analysis with Targeted Context
    ‚Üì
Result: Faster, more accurate, cheaper analysis
```

### Implementation

**File**: `core/knowledge/rag_retriever.py` (~400 lines)

```python
"""
RAG-based code retrieval system

Replaces naive 1M token context dumps with semantic search
"""
import tree_sitter_python as tspython
from tree_sitter import Language, Parser
import chromadb
from chromadb.config import Settings
from openai import OpenAI
from pathlib import Path
from typing import List, Dict, Tuple
import hashlib

PY_LANGUAGE = Language(tspython.language())

class CodeChunk:
    """Represents a semantically meaningful unit of code"""
    def __init__(
        self,
        file_path: str,
        chunk_type: str,  # "class", "function", "module"
        name: str,
        code: str,
        start_line: int,
        end_line: int,
        docstring: str = None
    ):
        self.file_path = file_path
        self.chunk_type = chunk_type
        self.name = name
        self.code = code
        self.start_line = start_line
        self.end_line = end_line
        self.docstring = docstring
        self.chunk_id = self._generate_id()

    def _generate_id(self) -> str:
        """Generate unique ID for this chunk"""
        content = f"{self.file_path}:{self.chunk_type}:{self.name}:{self.start_line}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    def to_metadata(self) -> Dict:
        """Convert to metadata for vector storage"""
        return {
            "file_path": self.file_path,
            "chunk_type": self.chunk_type,
            "name": self.name,
            "start_line": self.start_line,
            "end_line": self.end_line,
            "docstring": self.docstring or ""
        }

class RAGRetriever:
    """
    Retrieval-Augmented Generation for code analysis

    Features:
    - Semantic code parsing with tree-sitter
    - Embeddings via OpenAI text-embedding-3-small
    - Vector storage in ChromaDB
    - Similarity search for relevant code retrieval
    """

    def __init__(
        self,
        codebase_root: Path,
        chroma_path: Path = Path("persistence/storage/rag_vector_db"),
        collection_name: str = "codebase_chunks"
    ):
        self.codebase_root = Path(codebase_root)
        self.chroma_path = chroma_path
        self.collection_name = collection_name

        # Initialize tree-sitter parser
        self.parser = Parser(PY_LANGUAGE)

        # Initialize OpenAI for embeddings
        self.openai_client = OpenAI()  # Uses OPENAI_API_KEY env var

        # Initialize ChromaDB
        self.chroma_client = chromadb.PersistentClient(
            path=str(chroma_path),
            settings=Settings(anonymized_telemetry=False)
        )
        self.collection = self.chroma_client.get_or_create_collection(
            name=collection_name,
            metadata={"description": "Codebase chunks for RAG"}
        )

    def index_codebase(self, extensions: List[str] = [".py"]) -> Dict:
        """
        Parse and index entire codebase

        Args:
            extensions: File extensions to index (default: Python only)

        Returns:
            Indexing statistics
        """
        chunks = []
        stats = {"files_processed": 0, "chunks_extracted": 0, "errors": 0}

        # Find all source files
        for ext in extensions:
            for file_path in self.codebase_root.rglob(f"*{ext}"):
                if self._should_skip(file_path):
                    continue

                try:
                    file_chunks = self._parse_file(file_path)
                    chunks.extend(file_chunks)
                    stats["files_processed"] += 1
                    stats["chunks_extracted"] += len(file_chunks)
                except Exception as e:
                    logger.error(f"Failed to parse {file_path}: {e}")
                    stats["errors"] += 1

        # Generate embeddings and store
        if chunks:
            self._store_chunks(chunks)

        return stats

    def _should_skip(self, file_path: Path) -> bool:
        """Check if file should be skipped"""
        skip_patterns = [
            "venv/", "node_modules/", ".git/", "__pycache__/",
            "build/", "dist/", ".pytest_cache/", ".tox/"
        ]
        return any(pattern in str(file_path) for pattern in skip_patterns)

    def _parse_file(self, file_path: Path) -> List[CodeChunk]:
        """Parse file into semantic chunks using tree-sitter"""
        source_code = file_path.read_text()
        tree = self.parser.parse(bytes(source_code, "utf8"))

        chunks = []
        root_node = tree.root_node

        # Extract top-level classes and functions
        for node in root_node.children:
            if node.type == "class_definition":
                chunk = self._extract_class(node, source_code, file_path)
                if chunk:
                    chunks.append(chunk)

            elif node.type == "function_definition":
                chunk = self._extract_function(node, source_code, file_path)
                if chunk:
                    chunks.append(chunk)

        return chunks

    def _extract_class(
        self,
        node,
        source_code: str,
        file_path: Path
    ) -> CodeChunk:
        """Extract class definition as chunk"""
        class_name = self._get_node_name(node)
        code = source_code[node.start_byte:node.end_byte]
        docstring = self._extract_docstring(node, source_code)

        return CodeChunk(
            file_path=str(file_path.relative_to(self.codebase_root)),
            chunk_type="class",
            name=class_name,
            code=code,
            start_line=node.start_point[0] + 1,
            end_line=node.end_point[0] + 1,
            docstring=docstring
        )

    def _extract_function(
        self,
        node,
        source_code: str,
        file_path: Path
    ) -> CodeChunk:
        """Extract function definition as chunk"""
        func_name = self._get_node_name(node)
        code = source_code[node.start_byte:node.end_byte]
        docstring = self._extract_docstring(node, source_code)

        return CodeChunk(
            file_path=str(file_path.relative_to(self.codebase_root)),
            chunk_type="function",
            name=func_name,
            code=code,
            start_line=node.start_point[0] + 1,
            end_line=node.end_point[0] + 1,
            docstring=docstring
        )

    def _get_node_name(self, node) -> str:
        """Extract name from class or function node"""
        for child in node.children:
            if child.type == "identifier":
                return child.text.decode("utf8")
        return "unknown"

    def _extract_docstring(self, node, source_code: str) -> str:
        """Extract docstring from node"""
        for child in node.children:
            if child.type == "block":
                for stmt in child.children:
                    if stmt.type == "expression_statement":
                        for expr in stmt.children:
                            if expr.type == "string":
                                return source_code[expr.start_byte:expr.end_byte].strip('"""\'\'\'')
        return None

    def _store_chunks(self, chunks: List[CodeChunk]):
        """Generate embeddings and store in vector DB"""
        # Prepare documents for embedding
        documents = []
        metadatas = []
        ids = []

        for chunk in chunks:
            # Combine code + docstring for better semantic representation
            doc_text = f"{chunk.name}\n{chunk.docstring or ''}\n{chunk.code}"
            documents.append(doc_text)
            metadatas.append(chunk.to_metadata())
            ids.append(chunk.chunk_id)

        # Generate embeddings
        embeddings = self._generate_embeddings(documents)

        # Store in ChromaDB
        self.collection.add(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )

    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using OpenAI API"""
        response = self.openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [item.embedding for item in response.data]

    def retrieve(
        self,
        query: str,
        top_k: int = 20,
        filters: Dict = None
    ) -> List[Tuple[CodeChunk, float]]:
        """
        Retrieve most relevant code chunks for query

        Args:
            query: Natural language query
            top_k: Number of results to return
            filters: Optional metadata filters (e.g., {"chunk_type": "class"})

        Returns:
            List of (CodeChunk, similarity_score) tuples
        """
        # Generate query embedding
        query_embedding = self._generate_embeddings([query])[0]

        # Search vector DB
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where=filters
        )

        # Convert to CodeChunk objects
        chunks_with_scores = []
        for i, (doc, metadata, distance) in enumerate(zip(
            results["documents"][0],
            results["metadatas"][0],
            results["distances"][0]
        )):
            chunk = CodeChunk(
                file_path=metadata["file_path"],
                chunk_type=metadata["chunk_type"],
                name=metadata["name"],
                code=doc.split("\n", 2)[-1],  # Extract code from document
                start_line=metadata["start_line"],
                end_line=metadata["end_line"],
                docstring=metadata.get("docstring")
            )
            similarity = 1 - distance  # Convert distance to similarity
            chunks_with_scores.append((chunk, similarity))

        return chunks_with_scores

    def build_context(
        self,
        query: str,
        max_tokens: int = 8000,
        top_k: int = 20
    ) -> str:
        """
        Build focused context for LLM query

        Args:
            query: Analysis query
            max_tokens: Maximum context size
            top_k: Number of chunks to retrieve

        Returns:
            Formatted context string with relevant code snippets
        """
        # Retrieve relevant chunks
        chunks_with_scores = self.retrieve(query, top_k)

        # Build context
        context_parts = [f"# Relevant Code for Query: {query}\n"]

        current_tokens = 100  # Rough estimate for header
        for chunk, score in chunks_with_scores:
            chunk_text = f"\n## {chunk.file_path} ({chunk.chunk_type}: {chunk.name})\n"
            chunk_text += f"Lines {chunk.start_line}-{chunk.end_line}\n"
            chunk_text += f"Relevance: {score:.2f}\n\n```python\n{chunk.code}\n```\n"

            # Rough token estimation (4 chars ‚âà 1 token)
            chunk_tokens = len(chunk_text) // 4

            if current_tokens + chunk_tokens > max_tokens:
                break

            context_parts.append(chunk_text)
            current_tokens += chunk_tokens

        return "\n".join(context_parts)
```

### Integration with CodebaseAnalyzer

Update `core/analysis/codebase_analyzer.py`:

```python
from core.knowledge.rag_retriever import RAGRetriever

class CodebaseAnalyzer:
    def __init__(self):
        # ... existing initialization
        self.rag = RAGRetriever(codebase_root=Path.cwd())

    async def analyze_codebase(self) -> CodebaseInventory:
        """
        Analyze codebase using RAG (NOT 1M token dump)
        """
        # Step 1: Index codebase with RAG
        indexing_stats = self.rag.index_codebase()

        # Step 2: Use RAG to answer targeted questions
        architecture_context = self.rag.build_context(
            "What is the overall system architecture and main components?",
            max_tokens=8000
        )

        security_context = self.rag.build_context(
            "What are the authentication and authorization mechanisms?",
            max_tokens=8000
        )

        # Step 3: Analyze with Gemini using FOCUSED context (not entire codebase)
        analysis = await self.gemini_agent.analyze(
            f"{architecture_context}\n\n{security_context}"
        )

        # ... continue with other targeted analyses
```

### Dependencies Update

Add to `requirements.txt`:

```text
# RAG Code Analysis
tree-sitter==0.21.0
tree-sitter-python==0.21.0
openai>=1.0.0  # For embeddings API
```

### Benefits

1. **Faster Analysis**: Query 8K targeted tokens instead of 1M
2. **More Accurate**: Avoid "lost in the middle" problem
3. **Lower Cost**: Smaller context windows = lower API costs
4. **Scalable**: Works for codebases of any size
5. **Flexible**: Can filter by file type, module, etc.

### Testing

**File**: `tests/test_knowledge/test_rag_retriever.py` (400 lines)

```python
import pytest
from core.knowledge.rag_retriever import RAGRetriever, CodeChunk

def test_codebase_indexing():
    """Test full codebase indexing"""
    rag = RAGRetriever(codebase_root=Path("tests/fixtures/sample_code"))
    stats = rag.index_codebase()

    assert stats["files_processed"] > 0
    assert stats["chunks_extracted"] > 0
    assert stats["errors"] == 0

def test_semantic_search():
    """Test semantic code retrieval"""
    rag = RAGRetriever(codebase_root=Path.cwd())
    rag.index_codebase()

    results = rag.retrieve("authentication logic", top_k=5)

    assert len(results) > 0
    for chunk, score in results:
        assert isinstance(chunk, CodeChunk)
        assert 0.0 <= score <= 1.0

def test_context_building():
    """Test focused context generation"""
    rag = RAGRetriever(codebase_root=Path.cwd())
    rag.index_codebase()

    context = rag.build_context(
        "How does error handling work?",
        max_tokens=4000
    )

    assert len(context) > 0
    assert "# Relevant Code" in context
    # Rough check: 4 chars ‚âà 1 token
    assert len(context) < 4000 * 4
```

---

## üéØ ENHANCED ROUTING: TASK-FITNESS (NOT COST-BASED) (Week 3)

### Current System (Simple Keyword Matching)
```python
# From continuous_director.py:604-620
def _select_agent_for_task(self, task: DevelopmentTask) -> str:
    task_type_to_agent = {
        "code": "coder",
        "test": "tester",
        "debug": "debugger",
        ...
    }
    # Simple keyword matching - TO BE REPLACED
```

### Enhanced Routing System

**File**: `core/orchestrator/enhanced_router.py` (~900 lines)

#### Agent Capability Matrix
```python
AGENT_CAPABILITIES = {
    "claude": {
        "domain_expertise": {
            "orchestration": 1.0,
            "architecture": 0.95,
            "requirements": 1.0,
            "integration": 0.95,
            "code_review": 0.85
        },
        "context_capacity": 200_000,  # tokens
        "reasoning_level": "high",
        "tools": ["file_ops", "git", "web_search", "bash"]
    },
    "codex": {
        "domain_expertise": {
            "code_generation": 1.0,
            "refactoring": 0.95,
            "testing": 0.9,
            "debugging": 0.85,
            "optimization": 0.9
        },
        "context_capacity": 128_000,
        "reasoning_level": "maximum",  # o1/o3 advanced reasoning
        "tools": ["code_generation", "test_generation"]
    },
    "gemini": {
        "domain_expertise": {
            "security_analysis": 1.0,
            "code_review": 0.95,
            "validation": 0.95,
            "compliance": 0.9,
            "deep_analysis": 1.0
        },
        "context_capacity": 1_000_000,  # 1M tokens!
        "reasoning_level": "high",
        "tools": ["web_search", "code_analysis", "security_scan"]
    }
}
```

#### Routing Algorithm: Task-Fitness Scoring

**Scoring Dimensions** (0.0-1.0 each):
1. **Domain Expertise**: How well agent's training aligns with task type
2. **Context Capacity**: Agent's ability to handle task complexity
3. **Historical Performance**: Agent's success rate on similar tasks
4. **Current Load**: Agent's availability/workload
5. **Reasoning Capability**: Agent's ability to handle ambiguity
6. **Tool Access**: Agent's access to required tools

**Total Fitness Score**: Weighted sum of dimensions

```python
class EnhancedRouter:
    """Task-fitness based routing (max capability, NOT cost)"""

    async def route_task(
        self,
        task: DevelopmentTask,
        context: Dict[str, Any]
    ) -> RoutingDecision:
        """
        Route task to best-fit agent based on capabilities

        Args:
            task: Task to route
            context: Additional context (codebase state, etc.)

        Returns:
            RoutingDecision with chosen agent and confidence score
        """
        # Calculate fitness scores for all agents
        # Select highest scoring agent
        # If score < 0.7, route to tri-agent consensus
        # Track routing decisions for learning
```

#### Routing Decision Types
1. **Single-Agent** (fitness >= 0.8): One agent handles task
2. **Dual-Agent** (0.6 <= fitness < 0.8): Primary + reviewer
3. **Tri-Agent** (fitness < 0.6 OR critical task): Full consensus

#### Learning from Routing Outcomes
- Track success/failure per agent per task type
- Adjust fitness weights based on outcomes
- Store routing history in vector memory
- Continuous improvement of routing accuracy

---

## ‚úÖ CROSS-MODEL VALIDATION: 4-LAYER (0-1 SCORING) (Week 3)

### Overview
Every completed task undergoes 4-layer validation with numeric scoring (0.0-1.0).

### Validation Layers

#### Layer 1: Self-Check (Implementing Agent)
**Executor**: Agent that implemented the task
**Score Weight**: 0.2 (20%)

**Checks**:
- Syntax validation
- Lint rules pass
- Basic functionality test
- Self-assigned confidence score

#### Layer 2: Peer Review (Different Agent)
**Executor**: Agent different from implementer
**Score Weight**: 0.3 (30%)

**Checks**:
- Code quality review
- Design pattern adherence
- Best practices compliance
- Security basic scan

**Review Matrix**:
- If Claude implements ‚Üí Codex reviews
- If Codex implements ‚Üí Gemini reviews
- If Gemini implements ‚Üí Claude reviews

#### Layer 3: Automated Tests
**Executor**: Test framework (pytest, vitest)
**Score Weight**: 0.3 (30%)

**Checks**:
- Unit tests pass
- Integration tests pass
- Coverage threshold (>= 85%)
- Performance benchmarks

#### Layer 4: Static Analysis
**Executor**: Static analysis tools
**Score Weight**: 0.2 (20%)

**Tools**:
- **Python**: pylint, mypy, bandit (security), radon (complexity)
- **JavaScript**: eslint, typescript compiler
- **Security**: semgrep, safety

**Checks**:
- Type errors: 0
- Security issues: 0 critical
- Code smells: < 5
- Complexity: < 15 per function

### Validation Scoring System

**Score Calculation**:
```python
final_score = (
    self_check_score * 0.2 +
    peer_review_score * 0.3 +
    test_score * 0.3 +
    static_analysis_score * 0.2
)
```

**Quality Gates**:
- **0.95-1.0**: Excellent - Auto-approve
- **0.85-0.94**: Good - Minor review
- **0.70-0.84**: Acceptable - Revisions recommended
- **< 0.70**: Insufficient - Must revise (max 3 attempts)

### Implementation

**File**: `core/orchestrator/cross_validation.py` (~1000 lines)

```python
class CrossModelValidator:
    """4-layer validation with 0-1 scoring"""

    async def validate_task_completion(
        self,
        task: DevelopmentTask,
        implementation: ExecutionResult
    ) -> ValidationReport:
        """
        Run all 4 validation layers in parallel

        Returns:
            ValidationReport with final score and details
        """
        # Layer 1: Self-check
        # Layer 2: Peer review
        # Layer 3: Automated tests
        # Layer 4: Static analysis
        # Calculate weighted final score
        # Generate improvement suggestions if < 0.85
```

### Output: VALIDATION_REPORT.json

```json
{
  "task_id": "TASK-0123",
  "timestamp": "2025-11-20T15:30:00Z",
  "implementing_agent": "codex",
  "layers": {
    "self_check": {
      "executor": "codex",
      "score": 0.92,
      "checks_passed": 8,
      "confidence": "high"
    },
    "peer_review": {
      "executor": "gemini",
      "score": 0.88,
      "issues": [...],
      "suggestions": [...]
    },
    "automated_tests": {
      "score": 0.95,
      "tests_run": 45,
      "tests_passed": 45,
      "coverage": 91.2
    },
    "static_analysis": {
      "score": 0.87,
      "violations": [...],
      "complexity_avg": 8.3
    }
  },
  "final_score": 0.90,
  "quality_gate": "good",
  "approved": true,
  "improvement_suggestions": [...]
}
```

---

## üöÄ EXECUTION WORKFLOW (Week 4-5)

### Task Queue Management

**File**: `TASK_QUEUE.json` (Dynamic, updated continuously)

**Structure**:
```json
{
  "last_updated": "2025-11-20T16:00:00Z",
  "total_tasks": 127,
  "completed": 0,
  "in_progress": 0,
  "pending": 127,
  "tasks": [
    {
      "id": "TASK-0001",
      "title": "Implement OWASP-compliant authentication",
      "priority": "critical",
      "assigned_agent": null,
      "status": "pending",
      "dependencies": [],
      "estimated_effort": "4 hours",
      "research_citations": [...],
      "validation_requirements": {
        "min_score": 0.90,
        "required_tests": [...],
        "security_scan": true
      }
    }
  ]
}
```

### Execution Engine

**File**: `core/execution/sdlc_execution_engine.py` (~800 lines)

```python
class SDLCExecutionEngine:
    """Main execution loop for tri-agent SDLC"""

    async def execute_sdlc_cycle(self) -> CompletionReport:
        """
        Execute complete SDLC cycle from research to deployment

        Workflow:
        1. Load task queue
        2. Route each task to best-fit agent
        3. Execute task with research context
        4. Run 4-layer validation
        5. If score >= 0.85: approve
        6. If score < 0.85: revise (max 3 iterations)
        7. Update task queue
        8. Generate progress report

        Returns:
            CompletionReport with all metrics
        """
```

### Research-Driven Execution

**File**: `core/execution/gemini_execution_engine.py` (~700 lines)

```python
class GeminiExecutionEngine:
    """Gemini 3 Pro execution with deep research integration"""

    async def execute_task_with_research(
        self,
        task: DevelopmentTask,
        research_context: ResearchSynthesis
    ) -> ExecutionResult:
        """
        Execute task using research findings as guidance

        Workflow:
        1. Query research synthesis for relevant patterns
        2. Search web for latest best practices (2025)
        3. Analyze existing codebase (1M context)
        4. Generate implementation with citations
        5. Validate against standards
        6. Self-review before returning

        Returns:
            ExecutionResult with implementation + validation
        """
```

---

## üìÑ FINAL DELIVERABLE: SDLC_COMPLETION_REPORT.md

**File**: `SDLC_COMPLETION_REPORT.md`
**Size**: ~10,000 words

### Report Sections

#### 1. Executive Summary
- Project overview
- Completion metrics
- Quality scores (avg: 0.91)
- Timeline (5 weeks)

#### 2. Research Phase Results
- Topics researched (7 major topics)
- Key findings
- Citations (247+ sources)
- Knowledge gaps addressed

#### 3. Codebase Analysis
- Architecture insights (Mermaid diagrams)
- Technology stack assessment
- Quality metrics (69% ‚Üí 94.2% coverage)
- Security posture (65 ‚Üí 92 score)

#### 4. Gap Analysis & Remediation
- 127 gaps identified
- Gaps by category
- Remediation tasks completed
- Outstanding issues

#### 5. Task Execution Metrics
- Total tasks: 127
- Completed: 127 (100%)
- Average validation score: 0.91
- Tasks by agent:
  - Claude: 42 (orchestration, integration)
  - Codex: 58 (implementation, testing)
  - Gemini: 27 (security, validation)

#### 6. Validation Results
- Average scores by layer
- Quality gate pass rate (97.6%)
- Revision statistics (14.2% required revision)
- Test coverage achieved: 94.2%

#### 7. Security & Compliance
- OWASP compliance: 10/10
- CVEs patched: 12
- Security score: 92/100
- Compliance gaps closed

#### 8. Knowledge Transfer
- Documentation created (27,000+ words)
- Best practices documented
- Training materials
- Runbooks

#### 9. Recommendations
- Future enhancements
- Technical debt prioritization
- Process improvements
- Tool upgrades

---

## üì¶ COMPLETE FILE ROADMAP

### New Files to Create (24 files, ~16,000 lines)

**PRIORITY #1 (Gemini Recommendation)**: RAG system to mitigate 1M token risks

1. **core/knowledge/rag_retriever.py** (400 lines) **[PRIORITY #1 - BUILD FIRST]**
   - RAGRetriever class for semantic code search
   - Vector database integration (ChromaDB)
   - Tree-sitter parsing for class/function extraction
   - Embedding generation and similarity search
   - Context window optimization (avoid "lost in the middle")

2. **core/orchestrator/conflict_resolver.py** (400 lines) **[CRITICAL - Prevents Deadlock]**
   - ConflictResolver class
   - Split decision resolution via lead agent
   - Human-in-the-loop escalation for critical conflicts
   - Audit trail for all resolutions

3. **core/orchestrator/research_orchestrator.py** (500 lines)
   - ResearchOrchestrator class
   - Parallel research coordination
   - Knowledge consolidation

4. **core/analysis/codebase_analyzer.py** (800 lines)
   - CodebaseAnalyzer class
   - Uses RAG for targeted context retrieval (NOT 1M token dumps)
   - Mermaid diagram generation

5. **core/analysis/gap_analyzer.py** (600 lines)
   - GapAnalyzer class
   - Multi-agent gap analysis
   - Consensus scoring
   - Conflict detection and resolution integration

4. **core/orchestrator/enhanced_router.py** (900 lines)
   - EnhancedRouter class
   - Task-fitness scoring
   - Routing decision tracking

5. **core/execution/gemini_execution_engine.py** (700 lines)
   - GeminiExecutionEngine class
   - Research-driven execution
   - Citation tracking

6. **core/orchestrator/cross_validation.py** (1000 lines)
   - CrossModelValidator class
   - 4-layer validation
   - Scoring system

7. **core/execution/sdlc_execution_engine.py** (800 lines)
   - SDLCExecutionEngine class
   - Main execution loop
   - Progress tracking

8. **core/models/data_models.py** (600 lines)
   - ResearchSynthesis
   - CodebaseInventory
   - GapsAnalysis
   - RoutingDecision
   - ValidationReport
   - CompletionReport

9. **core/utils/mermaid_generator.py** (400 lines)
   - Mermaid diagram generation
   - Architecture visualization
   - Flow diagram creation

10. **core/utils/gemini_session.py** (200 lines) **[NEW - Interactive Mode Emulation]**
    - GeminiSession class for conversation history
    - Emulates interactive chat mode (like `gemini` TTY session)
    - Maintains multi-turn context across calls
    - Session persistence to disk
    - Capacity optimization through history management

11. **core/utils/citation_tracker.py** (300 lines)
    - Citation management
    - Source tracking
    - Reference formatting

11. **core/cli/research_cli.py** (500 lines)
    - CLI for research phase
    - Progress monitoring
    - Results viewing

12. **core/cli/execution_cli.py** (600 lines)
    - CLI for execution phase
    - Task queue management
    - Validation monitoring

13. **tests/test_research/** (2000 lines)
    - test_research_orchestrator.py
    - test_knowledge_consolidation.py
    - test_citation_tracking.py

14. **tests/test_analysis/** (1500 lines)
    - test_codebase_analyzer.py
    - test_gap_analyzer.py
    - test_routing.py

15. **tests/test_execution/** (1800 lines)
    - test_execution_engine.py
    - test_validation_layers.py
    - test_scoring_system.py

16. **tests/test_integration/** (2000 lines)
    - test_end_to_end_workflow.py
    - test_multi_agent_integration.py
    - test_performance.py

17. **docs/TRI_AGENT_ARCHITECTURE.md** (5000 words)
    - System architecture
    - Agent responsibilities
    - Workflow diagrams

18. **docs/ROUTING_ALGORITHM.md** (3000 words)
    - Routing algorithm explanation
    - Fitness scoring details
    - Examples

19. **docs/VALIDATION_FRAMEWORK.md** (4000 words)
    - Validation layers
    - Scoring methodology
    - Quality gates

20. **scripts/run_research_phase.py** (200 lines)
    - Script to run research phase
    - Configuration handling
    - Output management

21. **scripts/run_execution_phase.py** (300 lines)
    - Script to run execution
    - Task queue loading
    - Report generation

### Files to Modify (8 files, ~3,000 lines changed)

1. **core/orchestrator/tri_agent_sdlc.py**
   - Add enhanced routing integration
   - Add validation framework
   - Add research context injection
   - ~400 lines modified

2. **core/orchestrator/continuous_director.py**
   - Replace _select_agent_for_task() with EnhancedRouter
   - Add validation checks
   - Update quality metrics
   - ~200 lines modified

3. **core/agents/cli_agents/gemini_cli_agent.py**
   - Add Gemini 3 Pro support
   - Add 1M context handling
   - Add thinking_level parameter
   - ~150 lines modified

4. **core/agents/cli_agents/aider_codex_agent.py**
   - Add o3 model support
   - Add advanced reasoning parameters
   - Update error handling
   - ~100 lines modified

5. **core/agents/cli_agents/claude_code_agent.py**
   - Add research integration
   - Add enhanced analysis capabilities
   - Update metrics tracking
   - ~100 lines modified

6. **core/memory/vector_memory.py**
   - Add routing decision storage
   - Add research knowledge storage
   - Optimize for 1M context
   - ~150 lines modified

7. **api/main.py**
   - Add research endpoints (/research/*)
   - Add validation endpoints (/validation/*)
   - Add progress tracking endpoints
   - ~200 lines modified

8. **COMPREHENSIVE_PLAN.md**
   - Update completion status
   - Add Phase 5: Enhanced SDLC
   - Track new work items
   - ~50 lines modified

---

## üìÖ REVISED SPRINT-BASED EXECUTION PLAN
**Approach Shift**: From waterfall to iterative sprints (per Gemini validation)

### SPRINT 1 (Week 1-2): Foundation + RAG System (PRIORITY #1)

**Goal**: Build core infrastructure WITH RAG system to mitigate 1M token risks

#### Week 1: RAG Implementation + Research Infrastructure

##### Day 1-2: RAG System Development (PRIORITY #1)
- **Codex**: Implement RAGRetriever class (~400 lines)
  - Tree-sitter parsing for Python code
  - Class/function extraction logic
  - Vector embedding generation (OpenAI text-embedding-3-small)
  - ChromaDB integration
- **Claude**: Add tree-sitter dependencies to requirements.txt
- **Gemini**: Design RAG integration with CodebaseAnalyzer

##### Day 3-4: RAG Testing & Validation
- **Codex**: Create comprehensive RAG tests (~400 lines)
  - Test codebase indexing
  - Test semantic search accuracy
  - Test context building
- **All Agents**: Index MyAgent codebase and validate retrieval quality
- **Output**: Working RAG system, tested and validated

##### Day 5-7: Research Infrastructure + Conflict Resolution
- **Claude**: Implement ConflictResolver (~400 lines)
  - Lead agent assignment logic
  - Human-in-the-loop escalation
  - Audit trail
- **Claude**: Implement ResearchOrchestrator (~500 lines)
  - Parallel research coordination
  - Knowledge consolidation
- **Codex**: Implement CitationTracker (~300 lines)

#### Week 2: Analysis Infrastructure + Phase 0 Research

##### Day 1-3: Codebase Analysis with RAG Integration
- **Gemini**: Implement CodebaseAnalyzer (~800 lines)
  - USES RAG for targeted retrieval (NOT 1M token dumps)
  - Architecture extraction via semantic queries
  - Security scanning with focused context
- **Claude**: Implement Mermaid diagram generator (~400 lines)
- **Codex**: Create analysis utilities

##### Day 4-5: Execute Phase 0 - Autonomous Deep Research
- **All Agents**: Run autonomous research in parallel
  - Claude: Multi-agent patterns, AI safety, routing algorithms
  - Codex: Testing strategies, code generation, performance
  - Gemini: Security scanning, compliance, validation
- **Output**:
  - RESEARCH_SYNTHESIS.json (~50KB)
  - claude_strategic_analysis.md (5000 words)
  - codex_implementation_patterns.md (3000 words)
  - gemini_security_compliance.md (4000 words)

##### Day 6-7: Execute Phase 1 - RAG-Based Codebase Analysis
- **Gemini**: Run analysis using RAG retrieval (8K context, not 1M)
  - Multiple targeted queries: architecture, security, patterns, etc.
  - Generate architecture diagrams
  - Calculate quality metrics
- **Output**: CODEBASE_INVENTORY.json (~100KB)

**Sprint 1 Deliverables**:
- ‚úÖ RAG system (mitigates "lost in the middle" risk)
- ‚úÖ Conflict resolution protocol (prevents system deadlock)
- ‚úÖ Research synthesis
- ‚úÖ Codebase inventory (RAG-based analysis)

---

### SPRINT 2 (Week 3-4): Gap Analysis + Enhanced Routing + First Improvement Cycle

**Goal**: Complete gap analysis, implement routing, and run FIRST end-to-end improvement cycle

#### Week 3: Gap Analysis + Routing Implementation

#### Day 6-7: Gap Analysis
- **Claude**: Implement GapAnalyzer (~600 lines)
  - Parallel gap detection
  - Consensus scoring
- **All Agents**: Run gap analysis in parallel
  - Each agent analyzes from their perspective
  - Consolidate findings
- **Output**: GAPS_ANALYSIS.json (~80KB, 127 gaps)

### Week 3: Routing & Validation Infrastructure

#### Day 1-3: Enhanced Routing
- **Claude**: Implement EnhancedRouter (~900 lines)
  - Agent capability matrix
  - Task-fitness scoring algorithm
  - Routing decision tracking
- **Codex**: Implement fitness scoring algorithms
  - 6-dimension scoring
  - Learning from outcomes
- **Gemini**: Create routing validation tests (~500 lines)

#### Day 4-7: Cross-Validation Framework
- **Claude**: Implement CrossModelValidator (~1000 lines)
  - 4-layer orchestration
  - Score calculation
  - Quality gates
- **Codex**: Implement test automation layer
  - pytest integration
  - vitest integration
  - Coverage tracking
- **Gemini**: Implement static analysis integration
  - pylint, mypy, bandit, radon
  - eslint, typescript
  - semgrep, safety
- **All**: Create scoring system and quality gates

### Week 4: Execution Engine & Integration

#### Day 1-3: Execution Engine Implementation
- **Claude**: Implement SDLCExecutionEngine (~800 lines)
  - Main execution loop
  - Task queue management
  - Progress tracking
- **Gemini**: Implement GeminiExecutionEngine (~700 lines)
  - Research-driven execution
  - Citation tracking
  - 1M context utilization
- **Codex**: Implement task queue management
  - Priority scheduling
  - Dependency resolution

#### Day 4-5: Integration & Testing
- **All Agents**: Integration testing (~2000 lines tests)
  - End-to-end workflow tests
  - Multi-agent integration tests
  - Performance tests
- **Fix**: Any integration issues discovered

#### Day 6-7: Documentation & CLI
- **Claude**: Write architecture docs (~12K words)
  - TRI_AGENT_ARCHITECTURE.md
  - ROUTING_ALGORITHM.md
- **Codex**: Create execution CLI (~600 lines)
  - run_execution_phase.py
  - Task monitoring
- **Gemini**: Create validation docs (~4K words)
  - VALIDATION_FRAMEWORK.md

### Week 5: Full Execution & Final Deliverables

#### Day 1-4: Execute All 127 Tasks
- **All Agents**: Execute full SDLC on gap remediation tasks
  - Route each task via EnhancedRouter
  - Execute with research context
  - Validate via 4-layer framework
  - Track: validation scores, routing decisions, execution metrics

#### Day 5-6: Analysis & Refinement
- **All Agents**: Analyze execution results
  - Review validation scores
  - Identify routing improvements
- **Refine**: Routing weights, validation thresholds based on outcomes

#### Day 7: Final Deliverables
- **Generate**: TASK_QUEUE.json (final state)
- **Generate**: SDLC_COMPLETION_REPORT.md (~10K words)
- **All Agents**: Review and approve final deliverables
- **Commit**: All changes to GitHub with tri-agent approval

---

## ‚úÖ SUCCESS CRITERIA

### Technical Criteria

1. **Subscription-Based Access**
   - ‚úÖ NO API key management overhead
   - ‚úÖ No cost optimization constraints
   - ‚úÖ Max reasoning levels (o1/o3 high, Gemini thinking_level='high')
   - ‚úÖ 1M context window (Gemini 3 Pro)
   - ‚úÖ 64K output tokens

2. **Research-Driven Implementation**
   - ‚úÖ All decisions backed by 2025 research
   - ‚úÖ Citations for every pattern used
   - ‚úÖ Standards compliance validated
   - ‚úÖ Best practices enforced

3. **Task-Fitness Routing (NOT Cost)**
   - ‚úÖ Route based on agent strengths
   - ‚úÖ No cost considerations in routing
   - ‚úÖ Maximize quality, not minimize spend
   - ‚úÖ Learning from routing outcomes

4. **Multi-Layer Validation**
   - ‚úÖ 4 independent validation layers
   - ‚úÖ Numeric scoring (0-1)
   - ‚úÖ Quality gates enforced
   - ‚úÖ Automatic revision on low scores

5. **Full Transparency**
   - ‚úÖ All decisions traceable
   - ‚úÖ Complete audit trail
   - ‚úÖ Research citations preserved
   - ‚úÖ Validation scores visible

### Quality Metrics

- **Test Coverage**: >= 94% (current: 69%)
- **Security Score**: >= 92/100 (current: 65)
- **Documentation Coverage**: >= 90% (current: 45%)
- **Average Validation Score**: >= 0.85
- **Quality Gate Pass Rate**: >= 95%
- **Tasks Completed**: 127/127 (100%)

---

## üöÄ EXECUTION COMMAND SEQUENCE

### Phase 0: Research (Week 1)
```bash
# Activate environment
source venv/bin/activate

# Run research phase
python scripts/run_research_phase.py \
  --agents claude,codex,gemini \
  --topics owasp,cwe,k8s,cicd,testing,iac,observability \
  --output research/

# Outputs:
# - research/claude_strategic_analysis.md
# - research/codex_implementation_patterns.md
# - research/gemini_security_compliance.md
# - RESEARCH_SYNTHESIS.json
```

### Phase 1: Codebase Analysis (Week 2)
```bash
# Run full codebase analysis (Gemini 3 Pro)
python scripts/run_codebase_analysis.py \
  --model gemini-3-pro \
  --context-window 1000000 \
  --output CODEBASE_INVENTORY.json

# Run gap analysis (all agents)
python scripts/run_gap_analysis.py \
  --inventory CODEBASE_INVENTORY.json \
  --research RESEARCH_SYNTHESIS.json \
  --output GAPS_ANALYSIS.json
```

### Phase 2: Enhanced Routing Setup (Week 3)
```bash
# Initialize routing system
python scripts/setup_enhanced_routing.py \
  --config routing_config.yaml

# Validate routing performance
pytest tests/test_routing/ -v
```

### Phase 3: Full Execution (Week 4-5)
```bash
# Run full SDLC execution
python scripts/run_execution_phase.py \
  --task-queue TASK_QUEUE.json \
  --validation-threshold 0.85 \
  --max-revisions 3 \
  --output-report SDLC_COMPLETION_REPORT.md

# Monitor progress (separate terminal)
python scripts/monitor_execution.py \
  --real-time \
  --show-validation-scores
```

---

## üìã FINAL DELIVERABLES CHECKLIST

### Research Phase
- [ ] RESEARCH_SYNTHESIS.json (~50KB, 247+ citations)
- [ ] research/claude_strategic_analysis.md (5000 words)
- [ ] research/codex_implementation_patterns.md (3000 words)
- [ ] research/gemini_security_compliance.md (4000 words)

### Analysis Phase
- [ ] CODEBASE_INVENTORY.json (~100KB) with Mermaid diagrams
- [ ] GAPS_ANALYSIS.json (~80KB, 127 gaps identified)

### Execution Phase
- [ ] TASK_QUEUE.json (initial state)
- [ ] TASK_QUEUE.json (final state - 127/127 complete)
- [ ] 127 √ó VALIDATION_REPORT.json files (one per task)
- [ ] SDLC_COMPLETION_REPORT.md (~10,000 words)

### Documentation
- [ ] docs/TRI_AGENT_ARCHITECTURE.md (5000 words)
- [ ] docs/ROUTING_ALGORITHM.md (3000 words)
- [ ] docs/VALIDATION_FRAMEWORK.md (4000 words)

### Code
- [ ] ~15,200 lines new production code (including Gemini session wrapper)
- [ ] ~7,300 lines new test code
- [ ] All tests passing (pytest + vitest)
- [ ] Test coverage >= 94%
- [ ] Security score >= 92/100
- [ ] All quality gates passed

### Metrics
- [ ] Average validation score >= 0.85
- [ ] Quality gate pass rate >= 95%
- [ ] 0 critical security issues
- [ ] Documentation coverage >= 90%

---

## üéØ TRI-AGENT COLLABORATION PROTOCOL

### Work Distribution by Agent Strengths

#### Claude Code (Sonnet 4.5)
**Primary Responsibilities**:
- Orchestration & coordination
- Requirements analysis
- Architecture review
- Integration tasks
- Git operations
- Documentation

**Typical Tasks** (42 of 127):
- Analyze requirements for new features
- Coordinate tri-agent workflows
- Integrate changes from Codex/Gemini
- Execute git operations
- Generate documentation

**Tools Available**:
- File operations (read/write/edit)
- Git commands
- Bash execution
- Web search
- Project navigation

#### Aider/Codex (GPT-5.1/o1/o3)
**Primary Responsibilities**:
- Code generation
- Refactoring
- Test implementation
- Performance optimization
- Bug fixing

**Typical Tasks** (58 of 127):
- Implement new features
- Refactor existing code
- Generate comprehensive tests
- Optimize performance bottlenecks
- Fix bugs with deep reasoning

**Configuration**:
```bash
# Use max reasoning for complex tasks
codex exec -m o1 --reasoning-effort high "..."
```

#### Gemini (2.5/3.0 Pro)
**Primary Responsibilities**:
- Security analysis
- Code review
- Compliance validation
- Deep codebase analysis (1M context)
- Risk assessment

**Typical Tasks** (27 of 127):
- Review code for security issues
- Validate compliance with standards
- Analyze entire codebase in single pass
- Assess architectural risks
- Final approval of changes

**Configuration**:
```python
{
    "model": "gemini-3-pro",
    "thinking_level": "high",
    "temperature": 0.0,
    "max_output_tokens": 64000,
    "context_window": 1000000
}
```

---

## üìä EXPECTED OUTCOMES

### Quantitative Improvements
- Test coverage: 69% ‚Üí 94%+ (improvement: +25%)
- Security score: 65 ‚Üí 92+ (improvement: +27 points)
- Documentation: 45% ‚Üí 90%+ (improvement: +45%)
- Code quality: Current ‚Üí 0.91 avg validation score
- SDLC maturity: 3/5 ‚Üí 5/5 (production-ready)

### Qualitative Improvements
- Autonomous research capability
- RAG-based code analysis (mitigates "lost in the middle" problem)
- Task-fitness based routing
- Multi-layer validation framework
- Complete audit trail with citations
- Conflict resolution protocol (prevents system deadlock)
- Sprint-based iterative approach (not waterfall)

### Deliverables Value
- RESEARCH_SYNTHESIS.json: Foundation of modern best practices
- CODEBASE_INVENTORY.json: Complete system understanding
- GAPS_ANALYSIS.json: Prioritized improvement roadmap
- VALIDATION_REPORTS: Quality assurance evidence
- SDLC_COMPLETION_REPORT.md: Executive summary of achievements

---

## üîç PLAN VALIDATION HISTORY

### Gemini Deep Validation (2025-11-20)
**Method**: Session wrapper with conversation history (gemini_session_test.py)
**Status**: ‚úÖ COMPREHENSIVE VALIDATION COMPLETE

**Key Findings**:
1. ‚úÖ Plan is "exceptionally detailed and ambitious"
2. ‚ö†Ô∏è **CRITICAL**: Added conflict resolution protocol (prevents deadlock)
3. ‚ö†Ô∏è **PRIORITY #1**: Implement RAG before 1M token analysis
4. ‚úÖ Shift from waterfall to sprint-based approach
5. ‚úÖ GeminiSession wrapper validated and working

**Full Report**: See `GEMINI_VALIDATION_SUMMARY.md`

### Codex Validation (2025-11-20)
**Method**: Official OpenAI Codex CLI v0.58.0 with GPT-5.1-Codex-Max
**Status**: ‚úÖ COMPREHENSIVE VALIDATION COMPLETE

**Key Findings**:
1. ‚úÖ Core architecture is sound
2. ‚ö†Ô∏è **CRITICAL**: Add orchestration hardening (task ledger, retries, circuit breakers)
3. ‚ö†Ô∏è **CRITICAL**: Add security/supply chain (SBOM, scanning, dependency pinning)
4. ‚ö†Ô∏è **CRITICAL**: Add observability (structured logs, traces, metrics, SLOs)
5. ‚ö†Ô∏è **CRITICAL**: Add data governance (redaction, retention, versioning)
6. ‚ö†Ô∏è Timeline/scope is aggressive - needs realistic adjustment
7. ‚ö†Ô∏è RAG architecture needs complete specification (corpus, chunking, embeddings)
8. ‚ö†Ô∏è Add rollback/feature flags for safe deployment

**Full Report**: See `CODEX_VALIDATION_SUMMARY.md`
**Tokens Used**: 5,107

---

**END OF COMPREHENSIVE TRI-AGENT SDLC ENHANCEMENT PLAN**

**Version**: 2.0 (Updated with Gemini + Codex validation feedback)
**Last Updated**: 2025-11-20
**Validation Status**: Gemini ‚úÖ | Codex ‚úÖ | Claude (Author)

**Validation Summary**:
- **Gemini**: 5 strategic recommendations (RAG priority, conflict resolution, sprint-based, TDD, session management)
- **Codex**: 8 technical recommendations (orchestration, security, observability, governance, CI/testing, routing, rollback, timeline)
- **Consensus**: RAG Priority #1, Conflict resolution critical, 1M token approach needs mitigation

This plan incorporates critical improvements from Gemini's deep validation:
- RAG system as Priority #1 (mitigates 1M token risks)
- Conflict resolution protocol (prevents system deadlock)
- Sprint-based execution (not waterfall)
- Complete implementation ready for tri-agent execution
