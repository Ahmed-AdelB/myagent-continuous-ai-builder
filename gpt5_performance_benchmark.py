#!/usr/bin/env python3
"""
GPT-5 Enhanced System Performance Benchmark

Measures performance impact of all 7 GPT-5 improvements and provides
comprehensive performance analysis for the enhanced 22_MyAgent system.
"""

import asyncio
import sys
import os
import time
import json
import statistics
import psutil
import subprocess
import memory_profiler
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import tracemalloc

# Add project root for imports
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


@dataclass
class PerformanceMetric:
    """Individual performance measurement"""
    name: str
    value: float
    unit: str
    baseline: Optional[float]
    improvement: Optional[float]  # Percentage improvement vs baseline
    category: str  # Memory, CPU, Speed, Scalability
    timestamp: datetime


@dataclass
class BenchmarkResults:
    """Complete benchmark results"""
    overall_performance_score: float
    total_metrics: int
    improved_metrics: int
    degraded_metrics: int
    stable_metrics: int
    metrics: List[PerformanceMetric]
    system_baseline: Dict[str, float]
    gpt5_enhanced: Dict[str, float]
    performance_impact: Dict[str, float]
    recommendations: List[str]
    benchmark_duration: float
    timestamp: datetime


class GPT5PerformanceBenchmark:
    """Comprehensive performance benchmark for GPT-5 enhanced system"""

    def __init__(self):
        self.results = BenchmarkResults(
            overall_performance_score=0.0,
            total_metrics=0,
            improved_metrics=0,
            degraded_metrics=0,
            stable_metrics=0,
            metrics=[],
            system_baseline={},
            gpt5_enhanced={},
            performance_impact={},
            recommendations=[],
            benchmark_duration=0.0,
            timestamp=datetime.now()
        )
        self.start_time = datetime.now()

    def log(self, message: str, level: str = "INFO"):
        """Log benchmark message"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")

    def add_metric(self, name: str, value: float, unit: str, category: str,
                   baseline: Optional[float] = None):
        """Add a performance metric"""
        improvement = None
        if baseline is not None:
            if baseline > 0:
                improvement = ((baseline - value) / baseline) * 100 if "time" in unit.lower() else ((value - baseline) / baseline) * 100

            if improvement is not None:
                if abs(improvement) < 5:  # Less than 5% change
                    self.results.stable_metrics += 1
                    status = "STABLE"
                elif improvement > 0:
                    self.results.improved_metrics += 1
                    status = "IMPROVED"
                else:
                    self.results.degraded_metrics += 1
                    status = "DEGRADED"
            else:
                self.results.stable_metrics += 1
                status = "STABLE"
        else:
            self.results.stable_metrics += 1
            status = "NEW"

        metric = PerformanceMetric(
            name=name,
            value=value,
            unit=unit,
            baseline=baseline,
            improvement=improvement,
            category=category,
            timestamp=datetime.now()
        )

        self.results.metrics.append(metric)
        self.results.total_metrics += 1

        # Log result
        status_icon = "üöÄ" if status == "IMPROVED" else "üìä" if status == "STABLE" else "‚ö†Ô∏è" if status == "DEGRADED" else "üÜï"
        improvement_str = f" ({improvement:+.1f}%)" if improvement is not None else ""
        self.log(f"{status_icon} {name}: {value:.3f} {unit}{improvement_str}")

    async def measure_import_performance(self) -> Dict[str, float]:
        """Measure import performance for all GPT-5 components"""
        self.log("üì¶ Benchmarking import performance...")

        import_metrics = {}

        # Measure baseline Python imports
        start_time = time.perf_counter()
        import json
        import asyncio
        import datetime
        baseline_import_time = time.perf_counter() - start_time

        # Measure GPT-5 component imports
        gpt5_imports = [
            ("Meta-Governance", "core.governance.meta_governor"),
            ("Quality Framework", "core.evaluation.iteration_quality_framework"),
            ("Message Bus", "core.communication.agent_message_bus"),
            ("Memory Orchestrator", "core.memory.memory_orchestrator"),
            ("Review Gateway", "core.review.human_review_gateway"),
            ("RL Engine", "core.learning.reinforcement_learning_engine"),
            ("Modular Skills", "core.agents.modular_skills")
        ]

        total_import_time = 0
        successful_imports = 0

        for component_name, module_path in gpt5_imports:
            try:
                start_time = time.perf_counter()
                __import__(module_path)
                import_time = time.perf_counter() - start_time
                total_import_time += import_time
                successful_imports += 1

                import_metrics[f"{component_name.lower()}_import"] = import_time
                self.add_metric(f"{component_name} Import Time", import_time, "seconds", "Speed",
                               baseline_import_time / 7)  # Baseline per component

            except ImportError as e:
                self.log(f"‚ùå Failed to import {component_name}: {str(e)}", "ERROR")
                import_metrics[f"{component_name.lower()}_import"] = None

        # Overall import performance
        avg_import_time = total_import_time / successful_imports if successful_imports > 0 else float('inf')
        import_metrics["total_import_time"] = total_import_time
        import_metrics["average_import_time"] = avg_import_time

        self.add_metric("Total GPT-5 Import Time", total_import_time, "seconds", "Speed", 1.0)
        self.add_metric("Average Component Import Time", avg_import_time, "seconds", "Speed", 0.15)

        return import_metrics

    async def measure_memory_performance(self) -> Dict[str, float]:
        """Measure memory usage and efficiency"""
        self.log("üß† Benchmarking memory performance...")

        memory_metrics = {}
        process = psutil.Process()

        # Baseline memory usage
        baseline_memory = process.memory_info().rss / (1024 * 1024)  # MB
        memory_metrics["baseline_memory"] = baseline_memory

        # Memory usage after importing all GPT-5 components
        try:
            from core.governance.meta_governor import MetaGovernorAgent
            from core.evaluation.iteration_quality_framework import IterationQualityFramework
            from core.communication.agent_message_bus import AgentMessageBus
            from core.memory.memory_orchestrator import MemoryOrchestrator
            from core.review.human_review_gateway import HumanReviewGateway
            from core.learning.reinforcement_learning_engine import ReinforcementLearningEngine
            from core.agents.modular_skills import SkillRegistry

            post_import_memory = process.memory_info().rss / (1024 * 1024)  # MB
            memory_metrics["post_import_memory"] = post_import_memory
            import_memory_overhead = post_import_memory - baseline_memory

            self.add_metric("Import Memory Overhead", import_memory_overhead, "MB", "Memory", 50.0)

            # Memory usage during component instantiation
            instantiation_start_memory = post_import_memory

            # Create instances and measure memory
            instances = []
            try:
                from core.governance.meta_governor import GovernanceConfiguration
                instances.append(MetaGovernorAgent("benchmark", GovernanceConfiguration()))
            except Exception as e:
                self.log(f"Could not instantiate MetaGovernor: {e}", "WARN")

            try:
                from core.evaluation.iteration_quality_framework import QualityThresholds
                instances.append(IterationQualityFramework(QualityThresholds()))
            except Exception as e:
                self.log(f"Could not instantiate QualityFramework: {e}", "WARN")

            try:
                instances.append(AgentMessageBus())
            except Exception as e:
                self.log(f"Could not instantiate MessageBus: {e}", "WARN")

            try:
                instances.append(MemoryOrchestrator("benchmark"))
            except Exception as e:
                self.log(f"Could not instantiate MemoryOrchestrator: {e}", "WARN")

            post_instantiation_memory = process.memory_info().rss / (1024 * 1024)  # MB
            memory_metrics["post_instantiation_memory"] = post_instantiation_memory
            instantiation_memory_overhead = post_instantiation_memory - instantiation_start_memory

            self.add_metric("Instantiation Memory Overhead", instantiation_memory_overhead, "MB", "Memory", 100.0)

            # Memory efficiency (components per MB)
            if import_memory_overhead > 0:
                memory_efficiency = len(gpt5_imports) / import_memory_overhead
                self.add_metric("Memory Efficiency", memory_efficiency, "components/MB", "Memory", 0.1)

            # Clean up instances
            del instances

        except Exception as e:
            self.log(f"‚ùå Memory measurement failed: {str(e)}", "ERROR")
            memory_metrics["error"] = str(e)

        return memory_metrics

    async def measure_cpu_performance(self) -> Dict[str, float]:
        """Measure CPU performance and efficiency"""
        self.log("‚ö° Benchmarking CPU performance...")

        cpu_metrics = {}

        # Baseline CPU usage
        baseline_cpu = psutil.cpu_percent(interval=1)
        cpu_metrics["baseline_cpu"] = baseline_cpu

        # CPU usage during imports
        start_time = time.perf_counter()
        cpu_start = psutil.cpu_percent()

        try:
            # Import and instantiate components
            from core.governance.meta_governor import MetaGovernorAgent, GovernanceConfiguration
            from core.evaluation.iteration_quality_framework import IterationQualityFramework, QualityThresholds
            from core.agents.modular_skills import SkillRegistry, CodeGenerationSkill

            # Perform some operations
            governor = MetaGovernorAgent("cpu_test", GovernanceConfiguration())
            resources = governor.check_resource_usage()

            framework = IterationQualityFramework(QualityThresholds())
            registry = SkillRegistry()

            # Register and use a skill
            skill = CodeGenerationSkill()
            registry.register_skill(skill)

            cpu_end = psutil.cpu_percent()
            operation_time = time.perf_counter() - start_time

            cpu_usage_during_ops = (cpu_start + cpu_end) / 2
            cpu_efficiency = operation_time / max(cpu_usage_during_ops, 0.1)  # Avoid division by zero

            cpu_metrics["operation_cpu"] = cpu_usage_during_ops
            cpu_metrics["operation_time"] = operation_time
            cpu_metrics["cpu_efficiency"] = cpu_efficiency

            self.add_metric("CPU Usage During Operations", cpu_usage_during_ops, "%", "CPU", baseline_cpu)
            self.add_metric("Operation Execution Time", operation_time, "seconds", "Speed", 1.0)
            self.add_metric("CPU Efficiency", cpu_efficiency, "seconds/%", "CPU", 0.1)

        except Exception as e:
            self.log(f"‚ùå CPU measurement failed: {str(e)}", "ERROR")
            cpu_metrics["error"] = str(e)

        return cpu_metrics

    async def measure_scalability_performance(self) -> Dict[str, float]:
        """Measure scalability and concurrent performance"""
        self.log("üîÑ Benchmarking scalability performance...")

        scalability_metrics = {}

        try:
            from core.agents.modular_skills import SkillRegistry, SkillContext
            from core.agents.example_skills import CodeGenerationSkill, CodeAnalysisSkill

            # Test concurrent skill execution
            registry = SkillRegistry()
            skills = [CodeGenerationSkill(), CodeAnalysisSkill()]

            for skill in skills:
                registry.register_skill(skill)

            # Measure single execution
            context = SkillContext(
                task_description="Test task",
                input_data="test code",
                environment_state={},
                agent_capabilities={"text_processing"},
                available_resources={"cpu": 1.0, "memory": 1.0}
            )

            start_time = time.perf_counter()
            result = await skills[0].execute(context)
            single_execution_time = time.perf_counter() - start_time

            scalability_metrics["single_execution_time"] = single_execution_time
            self.add_metric("Single Skill Execution", single_execution_time, "seconds", "Speed", 0.5)

            # Measure concurrent execution
            start_time = time.perf_counter()
            tasks = [skill.execute(context) for skill in skills]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            concurrent_execution_time = time.perf_counter() - start_time

            successful_results = sum(1 for r in results if not isinstance(r, Exception))
            concurrency_efficiency = successful_results / len(skills)

            scalability_metrics["concurrent_execution_time"] = concurrent_execution_time
            scalability_metrics["concurrency_efficiency"] = concurrency_efficiency

            self.add_metric("Concurrent Execution Time", concurrent_execution_time, "seconds", "Scalability", single_execution_time)
            self.add_metric("Concurrency Efficiency", concurrency_efficiency, "ratio", "Scalability", 0.8)

            # Measure throughput
            if concurrent_execution_time > 0:
                throughput = successful_results / concurrent_execution_time
                scalability_metrics["throughput"] = throughput
                self.add_metric("Skill Execution Throughput", throughput, "ops/second", "Scalability", 1.0)

        except Exception as e:
            self.log(f"‚ùå Scalability measurement failed: {str(e)}", "ERROR")
            scalability_metrics["error"] = str(e)

        return scalability_metrics

    async def measure_integration_performance(self) -> Dict[str, float]:
        """Measure performance of integrated GPT-5 components"""
        self.log("üîó Benchmarking integration performance...")

        integration_metrics = {}

        try:
            # Measure cross-component communication
            from core.governance.meta_governor import MetaGovernorAgent, GovernanceConfiguration
            from core.evaluation.iteration_quality_framework import IterationQualityFramework, QualityThresholds
            from core.memory.memory_orchestrator import MemoryOrchestrator

            # Integration test 1: Governor + Quality Framework
            start_time = time.perf_counter()

            governor = MetaGovernorAgent("integration_test", GovernanceConfiguration())
            quality_framework = IterationQualityFramework(QualityThresholds())

            # Simulate iteration cycle
            governor.start_iteration("perf_test")
            test_metrics = {
                "test_coverage": {"line_coverage": 0.85, "pass_rate": 0.90},
                "performance": {"response_time": 0.2, "throughput": 800}
            }

            # Try to calculate quality score
            try:
                if hasattr(quality_framework, 'calculate_iteration_quality_score'):
                    quality_score = quality_framework.calculate_iteration_quality_score(test_metrics)
                elif hasattr(quality_framework, 'calculate_quality_score'):
                    quality_score = quality_framework.calculate_quality_score(test_metrics)
                else:
                    quality_score = 0.85  # Fallback

                governor.end_iteration("perf_test", {"quality_score": quality_score})
            except Exception as e:
                self.log(f"Quality calculation failed: {e}", "WARN")

            integration_time_1 = time.perf_counter() - start_time
            integration_metrics["governor_quality_integration"] = integration_time_1
            self.add_metric("Governor-Quality Integration", integration_time_1, "seconds", "Speed", 0.1)

            # Integration test 2: Memory operations
            start_time = time.perf_counter()
            try:
                orchestrator = MemoryOrchestrator("perf_test")
                # Simulate memory operations
                await asyncio.sleep(0.01)  # Simulate async operation
            except Exception as e:
                self.log(f"Memory integration failed: {e}", "WARN")

            integration_time_2 = time.perf_counter() - start_time
            integration_metrics["memory_integration"] = integration_time_2
            self.add_metric("Memory Integration", integration_time_2, "seconds", "Speed", 0.05)

            # Overall integration efficiency
            total_integration_time = integration_time_1 + integration_time_2
            integration_efficiency = 2 / total_integration_time  # 2 integrations
            integration_metrics["integration_efficiency"] = integration_efficiency
            self.add_metric("Integration Efficiency", integration_efficiency, "ops/second", "Scalability", 10.0)

        except Exception as e:
            self.log(f"‚ùå Integration measurement failed: {str(e)}", "ERROR")
            integration_metrics["error"] = str(e)

        return integration_metrics

    async def measure_system_overhead(self) -> Dict[str, float]:
        """Measure overall system overhead of GPT-5 enhancements"""
        self.log("üìä Measuring system overhead...")

        overhead_metrics = {}
        process = psutil.Process()

        # Baseline measurements
        baseline_memory = process.memory_info().rss / (1024 * 1024)
        baseline_cpu = psutil.cpu_percent(interval=0.5)

        # Load all GPT-5 components
        try:
            components_loaded = 0

            # Import all components
            imports = [
                "core.governance.meta_governor",
                "core.evaluation.iteration_quality_framework",
                "core.communication.agent_message_bus",
                "core.memory.memory_orchestrator",
                "core.review.human_review_gateway",
                "core.learning.reinforcement_learning_engine",
                "core.agents.modular_skills"
            ]

            for module_path in imports:
                try:
                    __import__(module_path)
                    components_loaded += 1
                except Exception as e:
                    self.log(f"Failed to load {module_path}: {e}", "WARN")

            # Measure post-load overhead
            post_load_memory = process.memory_info().rss / (1024 * 1024)
            post_load_cpu = psutil.cpu_percent(interval=0.5)

            memory_overhead = post_load_memory - baseline_memory
            cpu_overhead = post_load_cpu - baseline_cpu

            overhead_metrics["memory_overhead"] = memory_overhead
            overhead_metrics["cpu_overhead"] = cpu_overhead
            overhead_metrics["components_loaded"] = components_loaded

            self.add_metric("Total Memory Overhead", memory_overhead, "MB", "Memory", 200.0)
            self.add_metric("CPU Overhead", cpu_overhead, "%", "CPU", 10.0)

            # Overhead per component
            if components_loaded > 0:
                memory_per_component = memory_overhead / components_loaded
                self.add_metric("Memory per Component", memory_per_component, "MB/component", "Memory", 25.0)

            # System efficiency score
            if memory_overhead > 0 and components_loaded > 0:
                efficiency_score = components_loaded / memory_overhead  # Components per MB
                overhead_metrics["efficiency_score"] = efficiency_score
                self.add_metric("System Efficiency Score", efficiency_score, "components/MB", "Memory", 0.05)

        except Exception as e:
            self.log(f"‚ùå Overhead measurement failed: {str(e)}", "ERROR")
            overhead_metrics["error"] = str(e)

        return overhead_metrics

    def calculate_performance_score(self) -> float:
        """Calculate overall performance score"""
        if not self.results.metrics:
            return 0.0

        # Weight different categories
        category_weights = {
            "Speed": 0.3,
            "Memory": 0.25,
            "CPU": 0.25,
            "Scalability": 0.2
        }

        category_scores = {}
        category_counts = {}

        for metric in self.results.metrics:
            category = metric.category
            if category not in category_scores:
                category_scores[category] = 0.0
                category_counts[category] = 0

            # Convert metric to score (0-1)
            if metric.improvement is not None:
                # Use improvement percentage
                score = max(0, min(1, (metric.improvement + 100) / 200))  # -100% to +100% mapped to 0-1
            else:
                # Use normalized value based on category
                if category == "Speed" and "time" in metric.unit.lower():
                    score = max(0, min(1, 1 - (metric.value / 10)))  # Lower time is better
                elif category == "Memory" and "MB" in metric.unit:
                    score = max(0, min(1, 1 - (metric.value / 500)))  # Lower memory is better
                elif category == "CPU" and "%" in metric.unit:
                    score = max(0, min(1, 1 - (metric.value / 100)))  # Lower CPU is better
                else:
                    score = max(0, min(1, metric.value))  # Higher is better

            category_scores[category] += score
            category_counts[category] += 1

        # Calculate weighted average
        total_score = 0.0
        total_weight = 0.0

        for category, weight in category_weights.items():
            if category in category_scores and category_counts[category] > 0:
                avg_score = category_scores[category] / category_counts[category]
                total_score += avg_score * weight
                total_weight += weight

        return total_score / total_weight if total_weight > 0 else 0.0

    def generate_performance_recommendations(self) -> List[str]:
        """Generate performance optimization recommendations"""
        recommendations = []

        # Analyze metrics for issues
        slow_metrics = [m for m in self.results.metrics
                       if "time" in m.unit.lower() and m.value > 1.0]
        memory_heavy = [m for m in self.results.metrics
                       if "MB" in m.unit and m.value > 100]
        cpu_heavy = [m for m in self.results.metrics
                    if "%" in m.unit and m.value > 50]

        if slow_metrics:
            recommendations.append("‚ö° SPEED: Optimize slow operations")
            for metric in slow_metrics[:2]:
                recommendations.append(f"   - {metric.name}: {metric.value:.2f} {metric.unit}")

        if memory_heavy:
            recommendations.append("üß† MEMORY: Reduce memory consumption")
            for metric in memory_heavy[:2]:
                recommendations.append(f"   - {metric.name}: {metric.value:.1f} {metric.unit}")

        if cpu_heavy:
            recommendations.append("‚ö° CPU: Optimize CPU-intensive operations")
            for metric in cpu_heavy[:2]:
                recommendations.append(f"   - {metric.name}: {metric.value:.1f} {metric.unit}")

        # Performance score recommendations
        if self.results.overall_performance_score >= 0.8:
            recommendations.append("üéâ EXCELLENT: System performance is optimal")
        elif self.results.overall_performance_score >= 0.6:
            recommendations.append("‚úÖ GOOD: System performance is acceptable with room for optimization")
        else:
            recommendations.append("‚ö†Ô∏è NEEDS IMPROVEMENT: System performance requires optimization")

        # Specific improvements based on metrics
        improvement_metrics = [m for m in self.results.metrics if m.improvement and m.improvement > 10]
        if improvement_metrics:
            recommendations.append("üöÄ IMPROVEMENTS: Significant performance gains detected")
            for metric in improvement_metrics[:3]:
                recommendations.append(f"   + {metric.name}: {metric.improvement:.1f}% improvement")

        return recommendations

    async def run_comprehensive_benchmark(self) -> BenchmarkResults:
        """Run complete performance benchmark"""
        self.log("üöÄ Starting Comprehensive GPT-5 Performance Benchmark")
        self.log("=" * 80)

        # Run all benchmark tests
        benchmark_tasks = [
            ("Import Performance", self.measure_import_performance()),
            ("Memory Performance", self.measure_memory_performance()),
            ("CPU Performance", self.measure_cpu_performance()),
            ("Scalability Performance", self.measure_scalability_performance()),
            ("Integration Performance", self.measure_integration_performance()),
            ("System Overhead", self.measure_system_overhead())
        ]

        benchmark_data = {}
        for task_name, task_coro in benchmark_tasks:
            self.log(f"\nüìã Running {task_name}...")
            try:
                result = await task_coro
                benchmark_data[task_name] = result
                self.log(f"‚úÖ {task_name} completed")
            except Exception as e:
                benchmark_data[task_name] = {"error": str(e)}
                self.log(f"‚ùå {task_name} failed: {str(e)}", "ERROR")

        # Calculate overall performance score
        self.results.overall_performance_score = self.calculate_performance_score()

        # Generate recommendations
        self.results.recommendations = self.generate_performance_recommendations()

        # Store benchmark data
        self.results.system_baseline = {k: v for k, v in benchmark_data.items() if "error" not in v}
        self.results.gpt5_enhanced = benchmark_data

        # Calculate performance impact
        for category in ["Speed", "Memory", "CPU", "Scalability"]:
            category_metrics = [m for m in self.results.metrics if m.category == category]
            if category_metrics:
                avg_improvement = statistics.mean([m.improvement for m in category_metrics if m.improvement])
                self.results.performance_impact[category] = avg_improvement

        self.results.benchmark_duration = (datetime.now() - self.start_time).total_seconds()
        return self.results

    def print_benchmark_report(self):
        """Print comprehensive benchmark report"""
        duration = self.results.benchmark_duration

        print("\n" + "=" * 80)
        print("üéØ COMPREHENSIVE GPT-5 PERFORMANCE BENCHMARK REPORT")
        print("=" * 80)

        # Summary statistics
        print(f"üìä Overall Performance Score: {self.results.overall_performance_score:.3f}")
        print(f"üìà Metrics: {self.results.improved_metrics} improved, {self.results.stable_metrics} stable, {self.results.degraded_metrics} degraded")
        print(f"‚è±Ô∏è  Duration: {duration:.1f} seconds")

        # Performance by category
        print(f"\nüìä Performance by Category:")
        categories = set(m.category for m in self.results.metrics)
        for category in sorted(categories):
            category_metrics = [m for m in self.results.metrics if m.category == category]
            if category_metrics:
                avg_improvement = statistics.mean([m.improvement for m in category_metrics if m.improvement])
                metric_count = len(category_metrics)
                status = "üöÄ" if avg_improvement > 5 else "üìä" if avg_improvement > -5 else "‚ö†Ô∏è"
                print(f"   {status} {category}: {avg_improvement:+.1f}% average ({metric_count} metrics)")

        # Top performing metrics
        print(f"\nüèÜ Top Performing Metrics:")
        top_metrics = sorted([m for m in self.results.metrics if m.improvement],
                           key=lambda x: x.improvement, reverse=True)[:3]
        for metric in top_metrics:
            print(f"   üöÄ {metric.name}: {metric.improvement:+.1f}% improvement")

        # Performance impact summary
        if self.results.performance_impact:
            print(f"\nüìà Performance Impact Summary:")
            for category, impact in self.results.performance_impact.items():
                status = "üöÄ" if impact > 5 else "üìä" if impact > -5 else "‚ö†Ô∏è"
                print(f"   {status} {category}: {impact:+.1f}% average impact")

        # Recommendations
        if self.results.recommendations:
            print(f"\nüí° Performance Recommendations:")
            for rec in self.results.recommendations:
                print(f"   {rec}")

        print("\n" + "=" * 80)

    def save_benchmark_report(self):
        """Save detailed benchmark report"""
        report_data = asdict(self.results)
        report_file = f"gpt5_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(report_file, 'w') as f:
            json.dump(report_data, f, indent=2, default=str)

        print(f"üìã Detailed benchmark report saved: {report_file}")
        return report_file


async def main():
    """Main benchmark execution"""
    benchmark = GPT5PerformanceBenchmark()

    try:
        # Run comprehensive benchmark
        results = await benchmark.run_comprehensive_benchmark()

        # Print benchmark report
        benchmark.print_benchmark_report()

        # Save detailed report
        benchmark.save_benchmark_report()

        # Exit with appropriate code
        if results.overall_performance_score >= 0.8:
            print("\nüéâ PERFORMANCE BENCHMARK EXCELLENT - SYSTEM OPTIMIZED!")
            sys.exit(0)
        elif results.overall_performance_score >= 0.6:
            print(f"\n‚úÖ PERFORMANCE BENCHMARK GOOD - Score: {results.overall_performance_score:.3f}")
            sys.exit(0)
        else:
            print(f"\n‚ö†Ô∏è PERFORMANCE BENCHMARK NEEDS OPTIMIZATION - Score: {results.overall_performance_score:.3f}")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n‚ùå Benchmark cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• BENCHMARK FAILED: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())